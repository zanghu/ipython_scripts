{
 "metadata": {
  "name": "Convolutional_RBM_test"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import time\n",
      "import os\n",
      "from itertools import izip\n",
      "import copy\n",
      "\n",
      "import numpy\n",
      "import theano\n",
      "import theano.tensor as T\n",
      "\n",
      "from theano.tensor.shared_randomstreams import RandomStreams\n",
      "from theano.sandbox.rng_mrg import MRG_RandomStreams\n",
      "from theano.compat.python2x import OrderedDict\n",
      "\n",
      "from pylearn2.base import Block\n",
      "from pylearn2.models.model import Model\n",
      "from pylearn2.linear.matrixmul import MatrixMul\n",
      "from pylearn2.costs.cost import Cost, SumOfCosts\n",
      "from pylearn2.models.mlp import MLP, Layer\n",
      "from pylearn2.datasets.mnist import MNIST\n",
      "from pylearn2.space import VectorSpace, Conv2DSpace\n",
      "from pylearn2.linear.conv2d import Conv2D\n",
      "from pylearn2.linear.conv2d import make_random_conv2D, make_sparse_random_conv2D\n",
      "\n",
      "\n",
      "theano.config.compute_test_value = 'off'\n",
      "\n",
      "class HonglakLeeSparse(Cost):\n",
      "    \n",
      "    def __init__(self, p=0.02):\n",
      "        self.p = p\n",
      "    \n",
      "    def expr(self, model, data):\n",
      "        \n",
      "        v = data\n",
      "        p_h_given_v_matrix = model.propup(v)[-1]\n",
      "        sum_meta = T.square(self.p - T.mean(p_h_given_v_matrix, axis=0, dtype=theano.config.floatX))\n",
      "        expr = T.sum(sum_meta)\n",
      "        \n",
      "        return expr\n",
      "    \n",
      "    def get_data_specs(self, model):\n",
      "        return (model.get_input_space(), model.get_input_source())\n",
      "\n",
      "class MyContrastiveDivergence(Cost):\n",
      "    \n",
      "    def __init__(self, k, chain_num=None): # CD-k\n",
      "        # k: CD-k\n",
      "        self.k = k\n",
      "        self.chain_num = chain_num\n",
      "\t\n",
      "    def expr(self, model, data):\n",
      "        \n",
      "\t    return None\n",
      "        \n",
      "    def get_data_specs(self, model):\n",
      "        return model.get_monitoring_data_specs()\n",
      "\n",
      "class MyCD_energy_scan(MyContrastiveDivergence):\n",
      "    \n",
      "    def get_gradients(self, model, data, ** kwargs):\n",
      "        #print 'get_gradients'\n",
      "        pos_v = data\n",
      "        pos_h = model.sample_h_given_v(pos_v)[-1]\n",
      "        #chain_start = pos_v\n",
      "        h_samples = pos_h\n",
      "        #print 'v_samples', v_samples.ndim\n",
      "        [act_vis, vis_mfs, vis_samples, act_hids, hid_mfs, hid_samples], scan_updates = theano.scan(fn = model.gibbs_hvh, sequences=None, \n",
      "\t\t                        outputs_info=[None, None, None, None, None, h_samples], non_sequences=None, n_steps=self.k)\n",
      "        neg_v = vis_samples[-1]\n",
      "        neg_h = hid_samples[-1]\n",
      "        \n",
      "        cost = -(- model.energy(pos_v, pos_h).mean() + model.energy(neg_v, neg_h).mean())\n",
      "\n",
      "        params = list(model.get_params())\n",
      "\n",
      "        grads = T.grad(cost, params, disconnected_inputs = 'ignore', consider_constant=[pos_v, pos_h, neg_v, neg_h])\n",
      "\n",
      "        gradients = OrderedDict(izip(params, grads))\n",
      "\n",
      "        updates = OrderedDict()\n",
      "        \n",
      "        updates.update(scan_updates) # add scan_updates\n",
      "\n",
      "        return gradients, updates\n",
      "    \n",
      "class MyCD_free_energy_scan(MyContrastiveDivergence):\n",
      "    \n",
      "    def get_gradients(self, model, data, ** kwargs):\n",
      "        #print 'get_gradients'\n",
      "        pos_v = data\n",
      "        #pos_h = model.sample_h_given_v(pos_v)[-1]\n",
      "        #chain_start = pos_v\n",
      "        #h_samples = pos_h\n",
      "        #print 'v_samples', v_samples.ndim\n",
      "        [act_hids, hid_mfs, hid_samples, act_vis, vis_mfs, vis_samples], scan_updates = theano.scan(fn = model.gibbs_vhv, sequences=None, \n",
      "\t\t                        outputs_info=[None, None, None, None, None, pos_v], non_sequences=None, n_steps=self.k)\n",
      "        neg_v = vis_samples[-1]\n",
      "        #neg_h = hid_samples[-1]\n",
      "        \n",
      "        cost = -(- model.free_energy(pos_v).mean() + model.free_energy(neg_v).mean())\n",
      "\n",
      "        params = list(model.get_params())\n",
      "\n",
      "        grads = T.grad(cost, params, disconnected_inputs = 'ignore', consider_constant=[pos_v, neg_v])\n",
      "\n",
      "        gradients = OrderedDict(izip(params, grads))\n",
      "\n",
      "        updates = OrderedDict()\n",
      "        \n",
      "        updates.update(scan_updates) # add scan_updates\n",
      "\n",
      "        return gradients, updates\n",
      "\t\n",
      "class MyPCD_scan(MyContrastiveDivergence):\n",
      "    \n",
      "    def get_gradients(self, model, data, ** kwargs):\n",
      "        #print 'get_gradients'\n",
      "        chain_start = theano.shared(numpy.zeros(shape=(self.chain_num, model.n_vis), dtype=theano.config.floatX), name='chain_start', borrow=True)\n",
      "        \n",
      "        [act_hids, hid_mfs, hid_samples, act_vis, vis_mfs, vis_samples], scan_updates = theano.scan(fn = model.gibbs_vhv, sequences=None, \n",
      "\t\t                        outputs_info=[None, None, None, None, None, chain_start], non_sequences=None, n_steps=self.k)\n",
      "    \n",
      "        chain_end = vis_samples[-1]\n",
      "        scan_updates[chain_start] = chain_end\n",
      "        \n",
      "        pos_v = data \n",
      "        \n",
      "        cost = -(- model.free_energy(pos_v).mean() + model.free_energy(chain_end).mean())\n",
      "\n",
      "        params = list(model.get_params())\n",
      "\n",
      "        grads = T.grad(cost, params, disconnected_inputs = 'ignore', consider_constant=[pos_v, chain_end])\n",
      "\n",
      "        gradients = OrderedDict(izip(params, grads))\n",
      "\n",
      "        updates = OrderedDict()\n",
      "        \n",
      "        updates.update(scan_updates) # manual added\n",
      "\n",
      "        return gradients, updates\n",
      "\t\n",
      "# Is that necessary to inherit Layer class??\n",
      "class MyConvRBM(Model, Block):\n",
      "    \"\"\"Convolutional Restricted Boltzmann Machine (RBM)  \"\"\"\n",
      "    #batch_size\n",
      "    #kernel_shape = (kernelshape[0], kernel_shape[1])\n",
      "    #kernel_stride = (kernel_stride[0], kernel_stride[1])\n",
      "    #input_space: Conv2DSpace, channels, row, col\n",
      "    #detector_channels: channel number of detector layer\n",
      "    #pool_shape = (pool_shape[0], pool_shape[1])\n",
      "    #pool_stride = (pool_stride[0], pool_stride[1])\n",
      "    \n",
      "    #irange: 0.05\n",
      "    #input_space = input_space\n",
      "    #output_space\n",
      "    #kernel_shape = kernel_shape\n",
      "    #batch_size = batch_size\n",
      "    #subsample = kernel_stride\n",
      "    #border_mode = 'valid'(default)\n",
      "    def __init__(self, n_vis, n_hid, transformer=None, h_bias=None, v_bias=None, numpy_rng=None,theano_rng=None,\n",
      "                    irange=0.05, input_sapce=None, output_space=None, kernek_shape=None, batch_size=None, kernel_stride=(1, 1), border_model='valid'):\n",
      "        \n",
      "        Model.__init__(self) # self.names_to_del = set(); self._test_batch_size = 2\n",
      "        Block.__init__(self) # self.fn = None; self.cpu_only = False\n",
      "\n",
      "        self.n_vis = n_vis\n",
      "        self.n_hid = n_hid\n",
      "        \n",
      "        if transformer is not None:\n",
      "            assert isinstance(transformer, Conv2D)\n",
      "            input_space = transformer.input_space\n",
      "            output_axes = transformer.output_axes\n",
      "            output_shape = \n",
      "            \n",
      "        else:\n",
      "            transformer = make_random_conv2D(irange=irange, input_space=input_space, output_space=output_space, \n",
      "                                             kernel_shape=kernel_shape, batch_size=batch_size, \n",
      "        \n",
      "        self.input_space = (dim=self.n_vis) # add input_space\n",
      "        self.output_space = VectorSpace(dim=self.n_hid) # add output_space\n",
      "\n",
      "        if numpy_rng is None:\n",
      "            # create a number generator\n",
      "            numpy_rng = numpy.random.RandomState(seed=19900418)\n",
      "        self.numpy_rng = numpy_rng\n",
      "\n",
      "        if theano_rng is None:\n",
      "            theano_rng = RandomStreams(numpy_rng.randint(2 ** 30))\n",
      "        self.theano_rng = theano_rng\n",
      "\n",
      "        if W is None:\n",
      "            init_W = numpy.asarray(numpy_rng.uniform(\n",
      "                      low=-4 * numpy.sqrt(6. / (n_hid + n_vis)),\n",
      "                      high=4 * numpy.sqrt(6. / (n_hid + n_vis)),\n",
      "                      size=(n_vis, n_hid)),\n",
      "                      dtype=theano.config.floatX)\n",
      "            # theano shared variables for weights and biases\n",
      "            W = theano.shared(value=init_W, name='W', borrow=True)\n",
      "\n",
      "        if h_bias is None:\n",
      "            # create shared variable for hidden units bias\n",
      "            h_bias = theano.shared(value=numpy.zeros(n_hid, dtype=theano.config.floatX), name='h_bias', borrow=True)\n",
      "\n",
      "        if v_bias is None:\n",
      "            # create shared variable for visible units bias\n",
      "            v_bias = theano.shared(value=numpy.zeros(n_vis, dtype=theano.config.floatX), name='v_bias', borrow=True)\n",
      "\n",
      "        self.W = W\n",
      "        self.h_bias = h_bias\n",
      "        self.v_bias = v_bias\n",
      "\n",
      "        self._params = [self.W, self.h_bias, self.v_bias]\n",
      "        \n",
      "    def get_monitoring_data_specs(self):\n",
      "        \n",
      "\t    return (self.get_input_space(), self.get_input_source())\n",
      "\t\t\n",
      "    def get_monitoring_channels(self, data):\n",
      "        \n",
      "        v = data\n",
      "        channels = {}\n",
      "\t\t# recon_error\n",
      "        channel_name = 'recon_error'\n",
      "        p_v_given_h, v_sample = self.gibbs_vhv(v)[4:]\n",
      "        recon_error = ((p_v_given_h - v) ** 2).mean(axis=0).sum()\n",
      "        channels[channel_name] = recon_error\n",
      "        \n",
      "        return channels\n",
      "    \n",
      "    def energy(self, v, h):\n",
      "        W, c, b = self.get_params()\n",
      "        #energy = - (T.dot(v, b) + T.dot(h, c) + T.dot(T.dot(v, W), h.T) * T.eye(n=v.shape[0], m=h.shape[0]).sum(axis=0))\n",
      "        energy = - (T.dot(v, b) + T.dot(h, c) + (T.dot(v, W) * h).sum(axis=1))\n",
      "        #energy = - (T.dot(v, b) + T.dot(h, c) + T.dot((T.dot(v, W)).T, h))\n",
      "        return energy\n",
      "\n",
      "    def free_energy(self, v_sample):\n",
      "        #print 'free_energy'\n",
      "        ''' Function to compute the free energy '''\n",
      "        wx_b = T.dot(v_sample, self.W) + self.h_bias\n",
      "        #print 'wx_b', wx_b.ndim\n",
      "        v_bias_term = T.dot(v_sample, self.v_bias)\n",
      "        softplus_term = T.sum(T.nnet.softplus(wx_b), axis=1)\n",
      "        return - v_bias_term - softplus_term\n",
      "\n",
      "    def propup(self, vis):\n",
      "        pre_sigmoid_activation = T.dot(vis, self.W) + self.h_bias\n",
      "        return [pre_sigmoid_activation, T.nnet.sigmoid(pre_sigmoid_activation)]\n",
      "\n",
      "    def sample_h_given_v(self, v0_sample):\n",
      "        ''' This function infers state of hidden units given visible units '''\n",
      "        # compute the activation of the hidden units given a sample of\n",
      "        # the visibles\n",
      "        pre_sigmoid_h1, h1_mean = self.propup(v0_sample)\n",
      "        h1_sample = self.theano_rng.binomial(size=h1_mean.shape,\n",
      "                                             n=1, p=h1_mean,\n",
      "                                             dtype=theano.config.floatX)\n",
      "        return [pre_sigmoid_h1, h1_mean, h1_sample]\n",
      "\n",
      "    def propdown(self, hid):\n",
      "        pre_sigmoid_activation = T.dot(hid, self.W.T) + self.v_bias\n",
      "        return [pre_sigmoid_activation, T.nnet.sigmoid(pre_sigmoid_activation)]\n",
      "\n",
      "    def sample_v_given_h(self, h0_sample):\n",
      "        ''' This function infers state of visible units given hidden units '''\n",
      "        # compute the activation of the visible given the hidden sample\n",
      "        pre_sigmoid_v1, v1_mean = self.propdown(h0_sample)\n",
      "        v1_sample = self.theano_rng.binomial(size=v1_mean.shape,\n",
      "                                             n=1, p=v1_mean,\n",
      "                                             dtype=theano.config.floatX)\n",
      "        return [pre_sigmoid_v1, v1_mean, v1_sample]\n",
      "\n",
      "    def gibbs_hvh(self, h0_sample):\n",
      "        ''' This function implements one step of Gibbs sampling,\n",
      "            starting from the hidden state'''\n",
      "        pre_sigmoid_v1, v1_mean, v1_sample = self.sample_v_given_h(h0_sample)\n",
      "        pre_sigmoid_h1, h1_mean, h1_sample = self.sample_h_given_v(v1_sample)\n",
      "        return [pre_sigmoid_v1, v1_mean, v1_sample,\n",
      "                pre_sigmoid_h1, h1_mean, h1_sample]\n",
      "\n",
      "    def gibbs_vhv(self, v0_sample):\n",
      "        ''' This function implements one step of Gibbs sampling,\n",
      "            starting from the visible state'''\n",
      "        pre_sigmoid_h1, h1_mean, h1_sample = self.sample_h_given_v(v0_sample)\n",
      "        pre_sigmoid_v1, v1_mean, v1_sample = self.sample_v_given_h(h1_sample)\n",
      "        return [pre_sigmoid_h1, h1_mean, h1_sample, pre_sigmoid_v1, v1_mean, v1_sample]\n",
      "\t\t\t\t\n",
      "\t# interface for pylearn2.model.mlp PretraindLayer\n",
      "    def upward_pass(self, state_below):\n",
      "        return self.propup(state_below)[1]\n",
      "    \n",
      "    # default cost is cd-1\n",
      "    def get_default_cost(self):\n",
      "        return MyCD_free_energy_scan(k=1)\n",
      "    \n",
      "    def make_dataset(self, dataset,sample=False): # use rbm as a feature extractor, daatset pass through the filter and produce a new datset\n",
      "        \n",
      "        orin = T.matrix()\n",
      "        if sample == False:\n",
      "            f = theano.function([orin], self.propup(orin)[-1])\n",
      "        else:\n",
      "            f = theano.function([orin], self.sample_h_given_v(orin)[-1])\n",
      "        X_new = f(dataset.X)\n",
      "        new_ds = DatasetFromDesign(design_matrix=X_new, label=dataset.y)\n",
      "        #print new_ds.__dict__\n",
      "        #print X_new.shape\n",
      "        return new_ds\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    from pylearn2.datasets.mnist import MNIST\n",
      "    from pylearn2.training_algorithms.sgd import SGD\n",
      "    from pylearn2.train import Train\n",
      "    from pylearn2.termination_criteria import MonitorBased\n",
      "    from pylearn2.train_extensions.best_params import MonitorBasedSaveBest\n",
      "    from pylearn2.training_algorithms.sgd import MonitorBasedLRAdjuster\n",
      "    from pylearn2.training_algorithms.sgd import MomentumAdjustor\n",
      "    from pylearn2.termination_criteria import EpochCounter\n",
      "\t\n",
      "\t\n",
      "    dsm_train = MNIST(which_set='train', start=0, stop=50000, one_hot=True)\n",
      "    #dsm_valid = MNIST(which_set='train', start=500, stop=600, one_hot=True)\n",
      "    #dsm_test = MNIST(which_set='test', start=0, stop=100, one_hot=True)\n",
      "\t\n",
      "    #monitoring_dataset = {'train': dsm_train, 'valid': dsm_valid, 'test': dsm_test}\n",
      "    monitoring_dataset = {'train': dsm_train}\n",
      "\t\n",
      "    rbm_model = MyRBM(n_vis=784, n_hid=500)\n",
      "    \n",
      "    #cd_cost = MyCD_for(k=15)\n",
      "    #cd_cost = MyCD_scan(k=15)\n",
      "    #cd_cost = MyPCD_for(k=15, chain_num=20)\n",
      "    total_cost = MyPCD_scan(k=15, chain_num=20)\n",
      "    \n",
      "    #total_cost = MyCD_energy_scan(k=1)\n",
      "    #total_cost = SumOfCosts(costs=[MyCD_energy_scan(k=1), (1/ 0.05, HonglakLeeSparse(p=0.05))])\n",
      "    \n",
      "    #alg = SGD(learning_rate=0.1, cost=total_cost, batch_size=20, init_momentum=None, monitoring_dataset=monitoring_dataset,\n",
      "              #termination_criterion=MonitorBased(channel_name='valid_recon_error', N=10))\n",
      "    #          termination_criterion=EpochCounter(max_epochs=100))\n",
      "    \n",
      "    alg = SGD(learning_rate=0.1, cost=None, batch_size=20, init_momentum=None, monitoring_dataset=monitoring_dataset,\n",
      "              #termination_criterion=MonitorBased(channel_name='valid_recon_error', N=10))\n",
      "              termination_criterion=EpochCounter(max_epochs=100))\n",
      "    \n",
      "    # note: the save_path in extensions's parameters must not be identical to Train's save_path parameter, otherwise the overwrite problem will happen\n",
      "    # distiguish the action between Train's save_freq and MonitorBasedSaveBest which in Train's extenions prameter\n",
      "    \n",
      "    #MonitorBasedLRAdjuster(dataset_name='valid'),MomentumAdjustor(start=1, saturate=20, final_momentum=.99)\n",
      "    train = Train(dataset=dsm_train, model=rbm_model, algorithm=alg,\n",
      "            #extensions=[MonitorBasedSaveBest(channel_name='test_recon_error', save_path='my_rbm_1021.pkl')],\n",
      "            save_path='my_rbm_trainsave_1021.pkl',\n",
      "            save_freq=10)\n",
      "    \n",
      "    train.main_loop()\n",
      "    \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}