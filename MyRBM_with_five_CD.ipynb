{
 "metadata": {
  "name": "MyRBM_with_five_CD"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Binary-Binary RBM"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "\u4ec5\u4fdd\u7559\u4e24\u4e2a\u57fa\u672cCD\u7b97\u6cd5\uff1a1.\u9690\u5355\u5143\u503c\u57fa\u4e8e\u62bd\u6837\u7684CD\uff1b2.\u9690\u5355\u5143\u503c\u57fa\u4e8e\u5747\u503c\u7684CD\n",
      "\u52a0\u5165\u5bf9Honglak Lee07\u7684\u7a00\u758f\u60e9\u7f5a\u9879\u7684\u652f\u6301\uff0c\u7a00\u758f\u60e9\u7f5a\u9879\u5728\u8bad\u7ec3\u65f6\u53ea\u5f71\u54cdh_bias\u7684\u503c\uff0c\u548cGRBM\u4e2d\u7684\u8bbe\u7f6e\u4e00\u81f4."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import time\n",
      "import os\n",
      "from itertools import izip\n",
      "import copy\n",
      "\n",
      "import numpy\n",
      "import theano\n",
      "import theano.tensor as T\n",
      "\n",
      "from theano.tensor.shared_randomstreams import RandomStreams\n",
      "from theano.sandbox.rng_mrg import MRG_RandomStreams\n",
      "from theano.compat.python2x import OrderedDict\n",
      "\n",
      "from pylearn2.base import Block\n",
      "from pylearn2.models.model import Model\n",
      "from pylearn2.linear.matrixmul import MatrixMul\n",
      "from pylearn2.costs.cost import Cost, SumOfCosts\n",
      "from pylearn2.models.mlp import MLP, Layer\n",
      "from pylearn2.space import VectorSpace\n",
      "from pylearn2.utils import sharedX, as_floatX\n",
      "\n",
      "from module.dataset_from_design import DatasetFromDesign\n",
      "\n",
      "\n",
      "theano.config.compute_test_value = 'off'\n",
      "\n",
      "class HonglakLeeSparse(Cost):\n",
      "    \"\"\"Honglak Lee nips07\"\"\"\n",
      "    def __init__(self, p=0.02, lambda_=None):\n",
      "        \n",
      "        self.p = p\n",
      "        if lambda_ is None:\n",
      "            lambda_ = as_floatX(1. / self.p)\n",
      "        self.lambda_ = lambda_\n",
      "    \n",
      "    def expr(self, model, data):\n",
      "        \n",
      "        v = data\n",
      "        p_h_given_v_matrix = model.propup(v)[-1]\n",
      "        sum_meta = T.square(self.p - T.mean(p_h_given_v_matrix, axis=0, dtype=theano.config.floatX))\n",
      "        expr = T.sum(sum_meta)\n",
      "        \n",
      "        return expr\n",
      "    \n",
      "    def get_data_specs(self, model):\n",
      "        \n",
      "        return (model.get_input_space(), model.get_input_source())\n",
      "    \n",
      "    def get_gradients(self, model, data, ** kwargs):\n",
      "        \n",
      "        v = data\n",
      "        mean_matrix = model.propup(v)\n",
      "        #======================================================\n",
      "        part_j = self.p - mean_matrix.mean(axis=0)\n",
      "        part_i1_matrix = mean_matrix * (1. - mean_matrix)\n",
      "        #part_i = T.dot(v.T, part_i1_matrix)\n",
      "        #part_orin = part_i * part_j #\u77e9\u9635\u53f3\u4e58\u4e00\u4e2a\u884c\u5411\u91cf\n",
      "        #coeff_w = -2. *  v.shape[0]\n",
      "        #gW = coeff_w * part_orin #HL sparse\u9879\u4ea7\u751f\u7684\u68af\u5ea6\uff0c\u4e0d\u542blambda_\n",
      "        #=======================================================\n",
      "        \n",
      "        part_j1 = part_j\n",
      "        part_j2 = part_i1_matrix.mean(axis=0)\n",
      "        gc = -2. * part_j1 * part_j2\n",
      "\n",
      "        W, c, b = list(model.get_params())\n",
      "\n",
      "        #gradients = OrderedDict(izip([W, c], [1/self.p*gW, 1/self.p*gc]))\n",
      "        gradients = OrderedDict(izip([c], [as_floatX(1/self.p*gc)]))\n",
      "\n",
      "        updates = OrderedDict()\n",
      "\n",
      "        return gradients, updates\n",
      "         \n",
      "    \n",
      "\n",
      "class MyContrastiveDivergence(Cost):\n",
      "    \"\"\"\"CD\u7b97\u6cd5\u7684\u57fa\u7c7b\"\"\"\n",
      "    def __init__(self, k, chain_num=None): # CD-k\n",
      "        # k: CD-k\n",
      "        self.k = k\n",
      "        self.chain_num = chain_num\n",
      "\t\n",
      "    def expr(self, model, data):\n",
      "        #pos_v = data\n",
      "        #v_samples = pos_v\n",
      "        #for i in xrange(self.k):\n",
      "        #    v_samples = model.gibbs_vhv(v_samples)[-1]\n",
      "        #neg_v = v_samples\n",
      "        #neg_log_likelihood = -(- model.free_energy(pos_v).mean() + model.free_energy(neg_v).mean())\n",
      "        #return neg_log_likelihood\n",
      "\t    return None\n",
      "        \n",
      "    def get_data_specs(self, model):\n",
      "        return model.get_monitoring_data_specs()\n",
      "\n",
      "class MyCD_energy(MyContrastiveDivergence):\n",
      "    \"\"\"pos_h, neg_h\u5168\u90e8\u4f7f\u7528\u62bd\u6837\u503c\"\"\"\n",
      "    def get_gradients(self, model, data, ** kwargs):\n",
      "        #print 'get_gradients'\n",
      "        pos_v = data\n",
      "        pos_h = model.sample_h_given_v(pos_v)[0]\n",
      "        #print 'v_samples', v_samples.ndim\n",
      "        [v_mean, v_sample, h_mean, h_sample], scan_updates = theano.scan(fn = model.gibbs_hvh, sequences=None, \n",
      "\t\t                        outputs_info=[None, None, None, pos_h], non_sequences=None, n_steps=self.k)\n",
      "        neg_v = v_sample[-1]\n",
      "        neg_h = h_sample[-1]\n",
      "        \n",
      "        cost = -(- model.energy(pos_v, pos_h).mean() + model.energy(neg_v, neg_h).mean())\n",
      "\n",
      "        params = list(model.get_params())\n",
      "\n",
      "        grads = T.grad(cost, params, disconnected_inputs = 'ignore', consider_constant=[pos_v, pos_h, neg_v, neg_h])\n",
      "\n",
      "        gradients = OrderedDict(izip(params, grads))\n",
      "\n",
      "        updates = OrderedDict()\n",
      "        \n",
      "        updates.update(scan_updates) # add scan_updates\n",
      "\n",
      "        return gradients, updates\n",
      "    \n",
      "class MyCD_free_energy(MyContrastiveDivergence):\n",
      "    \"\"\"pos_h\uff0cneg_h\u5168\u90e8\u4f7f\u7528mean\u503c\"\"\"\n",
      "    def get_gradients(self, model, data, ** kwargs):\n",
      "        #print 'get_gradients'\n",
      "        pos_v = data\n",
      "        [h_mean, h_sample, v_mean, v_sample], scan_updates = theano.scan(fn = model.gibbs_vhv, sequences=None, \n",
      "\t\t                        outputs_info=[None, None, None, pos_v], non_sequences=None, n_steps=self.k)\n",
      "        neg_v = v_sample[-1]\n",
      "        \n",
      "        cost = -(- model.free_energy(pos_v).mean() + model.free_energy(neg_v).mean())\n",
      "\n",
      "        params = list(model.get_params())\n",
      "\n",
      "        grads = T.grad(cost, params, disconnected_inputs = 'ignore', consider_constant=[pos_v, neg_v])\n",
      "\n",
      "        gradients = OrderedDict(izip(params, grads))\n",
      "\n",
      "        updates = OrderedDict()\n",
      "        \n",
      "        updates.update(scan_updates) # add scan_updates\n",
      "\n",
      "        return gradients, updates\n",
      "\n",
      "\n",
      "\t\n",
      "# Is that necessary to inherit Layer class??\n",
      "class MyRBM(Model, Block):\n",
      "    \"\"\"Restricted Boltzmann Machine (RBM)  \"\"\"\n",
      "    def __init__(self, n_vis, n_hid, W=None, h_bias=None, v_bias=None, numpy_rng=None,theano_rng=None):\n",
      "        \"\"\"\"\"\"\n",
      "        Model.__init__(self) # self.names_to_del = set(); self._test_batch_size = 2\n",
      "        Block.__init__(self) # self.fn = None; self.cpu_only = False\n",
      "\n",
      "        self.n_vis = n_vis\n",
      "        self.n_hid = n_hid\n",
      "        \n",
      "        self.input_space = VectorSpace(dim=self.n_vis) # add input_space\n",
      "        self.output_space = VectorSpace(dim=self.n_hid) # add output_space\n",
      "\n",
      "        if numpy_rng is None:\n",
      "            # create a number generator\n",
      "            numpy_rng = numpy.random.RandomState(seed=19900418)\n",
      "        self.numpy_rng = numpy_rng\n",
      "\n",
      "        if theano_rng is None:\n",
      "            theano_rng = RandomStreams(numpy_rng.randint(2 ** 30))\n",
      "        self.theano_rng = theano_rng\n",
      "\n",
      "        if W is None:\n",
      "            init_W = numpy.asarray(numpy_rng.uniform(\n",
      "                      low=-4 * numpy.sqrt(6. / (n_hid + n_vis)),\n",
      "                      high=4 * numpy.sqrt(6. / (n_hid + n_vis)),\n",
      "                      size=(n_vis, n_hid)),\n",
      "                      dtype=theano.config.floatX)\n",
      "            # theano shared variables for weights and biases\n",
      "            W = theano.shared(value=init_W, name='W', borrow=True)\n",
      "\n",
      "        if h_bias is None:\n",
      "            # create shared variable for hidden units bias\n",
      "            h_bias = theano.shared(value=numpy.zeros(n_hid, dtype=theano.config.floatX), name='h_bias', borrow=True)\n",
      "\n",
      "        if v_bias is None:\n",
      "            # create shared variable for visible units bias\n",
      "            v_bias = theano.shared(value=numpy.zeros(n_vis, dtype=theano.config.floatX), name='v_bias', borrow=True)\n",
      "\n",
      "        self.W = W\n",
      "        self.h_bias = h_bias\n",
      "        self.v_bias = v_bias\n",
      "\n",
      "        self._params = [self.W, self.h_bias, self.v_bias]\n",
      "        \n",
      "    def get_monitoring_data_specs(self):\n",
      "        \"\"\"\"\"\"\n",
      "        return (self.get_input_space(), self.get_input_source())\n",
      "\t\t\n",
      "    def get_monitoring_channels(self, data):\n",
      "        \"\"\"\"\"\"\n",
      "        v = data\n",
      "        #H = self.sample_h_given_v(v)[1]\n",
      "        #h = H.mean(axis=0)\n",
      "        channels = {}\n",
      "\n",
      "        #channels =  { 'bias_hid_min' : T.min(self.h_bias),\n",
      "        #         'bias_hid_mean' : T.mean(self.h_bias),\n",
      "        #         'bias_hid_max' : T.max(self.h_bias),\n",
      "        #         'bias_vis_min' : T.min(self.v_bias),\n",
      "        #         'bias_vis_mean' : T.mean(self.v_bias),\n",
      "        #         'bias_vis_max': T.max(self.v_bias),\n",
      "        #         'h_min' : T.min(h),\n",
      "        #         'h_mean': T.mean(h),\n",
      "        #         'h_max' : T.max(h),\n",
      "                 #'W_min' : T.min(self.weights),\n",
      "                 #'W_max' : T.max(self.weights),\n",
      "                 #'W_norms_min' : T.min(norms),\n",
      "                 #'W_norms_max' : T.max(norms),\n",
      "                 #'W_norms_mean' : T.mean(norms),\n",
      "        #}\n",
      "\t\t# recon_error\n",
      "        channel_name = 'recon_error'\n",
      "        p_v_given_h, v_sample = self.gibbs_vhv(v)[2:]\n",
      "        recon_error = ((p_v_given_h - v) ** 2).mean(axis=0).sum()\n",
      "        channels[channel_name] = recon_error\n",
      "        \n",
      "        #pos_v = data\n",
      "        #[h_act, h_mean, h_sample, v_act, v_mean, v_sample], scan_updates = theano.scan(fn = self.gibbs_vhv, sequences=None, \n",
      "\t\t#                        outputs_info=[None, None, None, None, None, pos_v], non_sequences=None, n_steps=1)\n",
      "        #pos_h = h_sample[0]\n",
      "        #neg_v = v_sample[-1]\n",
      "        #neg_h = self.sample_h_given_v(v_sample[-1])[-1]\n",
      "        #cost = -(- self.energy(pos_v, pos_h).mean() + self.energy(neg_v, neg_h).mean())\n",
      "        #channels['energy_cost'] = cost\n",
      "        \n",
      "        #chain_start = theano.shared(numpy.zeros(shape=(20, self.n_vis), dtype=theano.config.floatX), name='chain_start', borrow=True)\n",
      "        #[h_act, h_mean, h_sample, v_act, v_mean, v_sample], scan_updates = theano.scan(fn = self.gibbs_vhv, sequences=None, \n",
      "\t\t#                        outputs_info=[None, None, None, None, None, chain_start], non_sequences=None, n_steps=1)\n",
      "        #chain_end = v_sample[-1]\n",
      "        #scan_updates[chain_start] = chain_end\n",
      "        #pos_v = data \n",
      "        #cost = -(- self.free_energy(pos_v).mean() + self.free_energy(chain_end).mean())\n",
      "        #channels['free_enegy_cost'] = cost\n",
      "        \n",
      "\t\t#pseudo_likelihood\n",
      "        #channel_name = 'pseudo_likelihood'\n",
      "        #bit_i_idx = theano.shared(value=0, name='bit_i_idx')\n",
      "        #xi = T.round(v)\n",
      "        #print 'xi',xi.ndim\n",
      "        #fe_xi = self.free_energy(xi)\n",
      "        #xi_flip = T.set_subtensor(xi[:, bit_i_idx], 1 - xi[:, bit_i_idx])\n",
      "        #print 'xi_flip', xi_flip.ndim\n",
      "        #fe_xi_flip = self.free_energy(xi_flip)\n",
      "        #cost = T.mean(self.n_vis * T.log(T.nnet.sigmoid(fe_xi_flip - fe_xi)))\n",
      "        #updates[bit_i_idx] = (bit_i_idx + 1) % self.n_vis\n",
      "        #channels[channel_name] = cost\n",
      "        \n",
      "        return channels\n",
      "    \n",
      "    def energy(self, v, h):\n",
      "        \"\"\"\u8ba1\u7b97\u80fd\u91cf\u51fd\u6570\"\"\"\n",
      "        W, c, b = self.get_params()\n",
      "        #energy = - (T.dot(v, b) + T.dot(h, c) + T.dot(T.dot(v, W), h.T) * T.eye(n=v.shape[0], m=h.shape[0]).sum(axis=0))\n",
      "        energy = - (T.dot(v, b) + T.dot(h, c) + (T.dot(v, W) * h).sum(axis=1))\n",
      "        #energy = - (T.dot(v, b) + T.dot(h, c) + T.dot((T.dot(v, W)).T, h))\n",
      "        return energy\n",
      "\n",
      "    def free_energy(self, v_sample):\n",
      "        #print 'free_energy'\n",
      "        ''' Function to compute the free energy '''\n",
      "        wx_b = T.dot(v_sample, self.W) + self.h_bias\n",
      "        #print 'wx_b', wx_b.ndim\n",
      "        v_bias_term = T.dot(v_sample, self.v_bias)\n",
      "        softplus_term = T.sum(T.nnet.softplus(wx_b), axis=1)\n",
      "        return - v_bias_term - softplus_term\n",
      "\n",
      "    def propup(self, v):\n",
      "        \"\"\"\u8ba1\u7b97\u6b63\u5411\u4f20\u64ad\u671f\u671b\"\"\"\n",
      "        sigmoid_activation = T.dot(v, self.W) + self.h_bias\n",
      "        return T.nnet.sigmoid(sigmoid_activation)\n",
      "\n",
      "    def sample_h_given_v(self, v0_sample):\n",
      "        ''' This function infers state of hidden units given visible units '''\n",
      "        # compute the activation of the hidden units given a sample of\n",
      "        # the visibles\n",
      "        h0_mean = self.propup(v0_sample)\n",
      "        h0_sample = self.theano_rng.binomial(n=1, p=h0_mean, dtype=theano.config.floatX)\n",
      "        return [h0_mean, h0_sample]\n",
      "\n",
      "    def propdown(self, h):\n",
      "        \"\"\"\u8ba1\u7b97\u53cd\u5411\u4f20\u64ad\u671f\u671b\"\"\"\n",
      "        sigmoid_activation = T.dot(h, self.W.T) + self.v_bias\n",
      "        return T.nnet.sigmoid(sigmoid_activation)\n",
      "\n",
      "    def sample_v_given_h(self, h0_sample):\n",
      "        ''' This function infers state of visible units given hidden units '''\n",
      "        # compute the activation of the visible given the hidden sample\n",
      "        v1_mean = self.propdown(h0_sample)\n",
      "        v1_sample = self.theano_rng.binomial(n=1, p=v1_mean,\n",
      "                                             dtype=theano.config.floatX)\n",
      "        return [v1_mean, v1_sample]\n",
      "\n",
      "    def gibbs_hvh(self, h0_sample):\n",
      "        ''' This function implements one step of Gibbs sampling,\n",
      "            starting from the hidden state'''\n",
      "        v1_mean, v1_sample = self.sample_v_given_h(h0_sample)\n",
      "        h1_mean, h1_sample = self.sample_h_given_v(v1_sample)\n",
      "        return [v1_mean, v1_sample, h1_mean, h1_sample]\n",
      "\n",
      "    def gibbs_vhv(self, v0_sample):\n",
      "        ''' This function implements one step of Gibbs sampling,\n",
      "            starting from the visible state'''\n",
      "        h0_mean, h0_sample = self.sample_h_given_v(v0_sample)\n",
      "        v1_mean, v1_sample = self.sample_v_given_h(h0_sample)\n",
      "        return [h0_mean, h0_sample, v1_mean, v1_sample]\n",
      "\t\t\t\t\n",
      "\t# interface for pylearn2.model.mlp PretraindLayer\n",
      "    def upward_pass(self, state_below):\n",
      "        \"\"\"\u7528\u6765\u517c\u5bb9mlp\"\"\"\n",
      "        return self.propup(state_below)\n",
      "    \n",
      "    def visual_config(self, step=1, dpi=240, start=0, total_num=100, v_shape=(28, 28), v_channels=1, random=False):\n",
      "        \"\"\"\"\"\"\n",
      "        assert total_num < self.n_hid - start\n",
      "        self.__dict__.update(locals())\n",
      "        del self.self\n",
      "    \n",
      "    #need to be cooperate with module.my_train_extensions.Visualizer(TrainExtension) instance\n",
      "    # which shoud be added to extensions prameter of pylearn2..train.Train\n",
      "    def get_visual(self):\n",
      "        #step: draw pictures every step epochs\n",
      "        #total_num: number of hidden units which need to be draw\n",
      "        #v_shape: shape of input image\n",
      "        #v_channels: number of channels that each input has\n",
      "        # random: if True, randomly select total_num hiddenunits, else use the first total_num hidden units\n",
      "        assert total_num < self.n_hid\n",
      "        save_path = os.getcwd() + '/visual_pic'\n",
      "        if not os.path.isdir(save_path):\n",
      "            os.mkdir(save_path)\n",
      "        if hasattr(self, 'epoch_cnt'):\n",
      "            self.epoch_cnt += 1\n",
      "        else:\n",
      "            self.epoch_cnt = 1\n",
      "        rng = self.numpy_rng\n",
      "        W = self.W.get_value()\n",
      "        v = W / ((W**2).sum(axis=1)).reshape(W.shape[0], 1)\n",
      "        \n",
      "        if (self.epoch_cnt - 1) % self.step != 0:\n",
      "            return None\n",
      "        \n",
      "        if self.random is True:\n",
      "            rl = rng.choice(xrange(self.n_hid), self.total_num)\n",
      "        else:\n",
      "            rl = range(self.total_num)\n",
      "        \n",
      "        l = int(numpy.ceil(numpy.sqrt(self.total_num)))\n",
      "\n",
      "        for i in xrange(l):\n",
      "            for j in xrange(l):\n",
      "                if self.v_channels == 1: #draw gray level img\n",
      "                    pylab.subplot(l, l, i*l+j+1); pylab.axis('off')\n",
      "                    pylab.imshow(v[:, rl[i*l+j]].reshape(self.v_shape), cmap=pylab.cm.gray)\n",
      "                else: # draw RGB img\n",
      "                    pylab.subplot(l, l, i*l+j+1); pylab.axis('off')\n",
      "                    pylab.imshow(v[:, rl[i*l+j]].reshape(3, self.v_shape[0], self.v_shape[1]).transpose(1,2,0))\n",
      "\n",
      "        pylab.savefig(save_path + '/epoch_' + str(self.epoch_cnt) + '.png', dpi=self.dpi) # save img\n",
      "        \n",
      "        return None\n",
      "        \n",
      "    # default cost is cd-1\n",
      "    def get_default_cost(self):\n",
      "        \"\"\"\"\"\"\n",
      "        return SumOfCosts(costs=[MyCD_free_energy(k=1), HonglakLeeSparse()])\n",
      "        #return MyCD_free_energy(k=1)\n",
      "        #return MyCD_energy(k=1)\n",
      "    \n",
      "    def make_dataset(self, dataset,sample=False): # use rbm as a feature extractor, daatset pass through the filter and produce a new datset\n",
      "        \n",
      "        orin = T.matrix()\n",
      "        if sample == False:\n",
      "            f = theano.function([orin], self.propup(orin)[-1])\n",
      "        else:\n",
      "            f = theano.function([orin], self.sample_h_given_v(orin)[-1])\n",
      "        X_new = f(dataset.X)\n",
      "        new_ds = DatasetFromDesign(design_matrix=X_new, label=dataset.y)\n",
      "        #print new_ds.__dict__\n",
      "        #print X_new.shape\n",
      "        return new_ds\n",
      "    \n",
      "    def get_sparseness_theano(W):\n",
      "        \"\"\"\u8ba1\u7b97\u77e9\u9635W\u6bcf\u4e00\u5217\u7684\u7a00\u758f\u7a0b\u5ea6\u7684\u5e73\u5747\u503c\uff0c\u8fd4\u56de\u4ee3\u8868\u7a00\u758f\u5ea6\u7684\u7b26\u53f7\u53d8\u91cf\"\"\"\n",
      "        n = W.shape[0]\n",
      "        W = self.W.T\n",
      "        sparseness_vector = (T.sqrt(n) - T.sum(T.abs_(W), axis=1) / T.sqrt(T.sum(W**2, axis=1))) / (T.sqrt(n) - 1.)\n",
      "        assert sparseness_vector.ndim == 1\n",
      "        return T.mean(sparseness_vector)\n",
      "    \n",
      "    def get_sparseness_numpy(h):\n",
      "        \"\"\"\u8ba1\u7b97\u77e9\u9635W\u6bcf\u4e00\u5217\u7684\u7a00\u758f\u7a0b\u5ea6\u7684\u5e73\u5747\u503c\uff0c\u8fd4\u56de\u4ee3\u8868\u7a00\u758f\u5ea6\u7684\u6570\u503c\"\"\"\n",
      "        n = W.shape[0]\n",
      "        W = self.W.T\n",
      "        sparseness_vector = (numpy.sqrt(n) - numpy.sum(numpy.abs(W), axis=1) / numpy.sqrt(numpy.sum(W**2, axis=1))) / (numpy.sqrt(n) - 1.)\n",
      "        assert sparseness_vector.ndim == 1\n",
      "        return numpy.mean(sparseness_vector)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "RBM\u63a7\u5236\u6587\u4ef6\u793a\u4f8b"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "if __name__ == '__main__':\n",
      "    from pylearn2.datasets.mnist import MNIST\n",
      "    from pylearn2.training_algorithms.sgd import SGD\n",
      "    from pylearn2.train import Train\n",
      "    from pylearn2.termination_criteria import MonitorBased\n",
      "    from pylearn2.train_extensions.best_params import MonitorBasedSaveBest\n",
      "    from pylearn2.training_algorithms.sgd import MonitorBasedLRAdjuster\n",
      "    from pylearn2.training_algorithms.sgd import MomentumAdjustor\n",
      "    from pylearn2.termination_criteria import EpochCounter\n",
      "    from pylearn2.costs.cost import SumOfCosts\n",
      "\t\n",
      "\t\n",
      "    dsm_train = MNIST(which_set='train', start=0, stop=500, one_hot=True)\n",
      "    #dsm_valid = MNIST(which_set='train', start=500, stop=600, one_hot=True)\n",
      "    #dsm_test = MNIST(which_set='test', start=0, stop=100, one_hot=True)\n",
      "\t\n",
      "    #monitoring_dataset = {'train': dsm_train, 'valid': dsm_valid, 'test': dsm_test}\n",
      "    monitoring_dataset = {'train': dsm_train}\n",
      "\t\n",
      "    rbm_model = MyRBM(n_vis=784, n_hid=500)\n",
      "\n",
      "    #cd_cost = MyCD_scan(k=15)\n",
      "    #total_cost = MyCD_energy_scan(k=1)\n",
      "    total_cost = SumOfCosts(costs=[MyCD_free_energy(k=1), HonglakLeeSparse()])\n",
      "    \n",
      "    #alg = SGD(learning_rate=0.1, cost=total_cost, batch_size=20, init_momentum=None, monitoring_dataset=monitoring_dataset,\n",
      "              #termination_criterion=MonitorBased(channel_name='valid_recon_error', N=10))\n",
      "    #          termination_criterion=EpochCounter(max_epochs=100))\n",
      "    \n",
      "    alg = SGD(learning_rate=0.01, cost=total_cost, batch_size=20, init_momentum=None, monitoring_dataset=monitoring_dataset,\n",
      "              #termination_criterion=MonitorBased(channel_name='valid_recon_error', N=10))\n",
      "              termination_criterion=EpochCounter(max_epochs=15))\n",
      "    \n",
      "    #MonitorBasedLRAdjuster(dataset_name='valid'),MomentumAdjustor(start=1, saturate=20, final_momentum=.99)\n",
      "    train = Train(dataset=dsm_train, model=rbm_model, algorithm=alg,\n",
      "            #extensions=[MonitorBasedSaveBest(channel_name='test_recon_error', save_path='my_rbm_1021.pkl')],\n",
      "            save_path='my_rbm_trainsave_1021.pkl',\n",
      "            save_freq=10)\n",
      "    t0 = time.clock()\n",
      "    train.main_loop()\n",
      "    t1 = time.clock()\n",
      "    print 'time elapsed on training is', t1 - t0\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Parameter and initial learning rate summary:\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tW: 0.01\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\th_bias: 0.01\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tv_bias: 0.01\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Compiling sgd_update...\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Compiling sgd_update done. Time elapsed: 3.000000 seconds\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "compiling begin_record_entry...\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "compiling begin_record_entry done. Time elapsed: 0.000000 seconds\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Monitored channels: \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tlearning_rate\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tmonitor_seconds_per_epoch\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\ttrain_recon_error\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\ttrain_term_1\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Compiling accum...\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "graph size: 21\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Compiling accum done. Time elapsed: 1.000000 seconds\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Monitoring step:\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tEpochs seen: 0\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tBatches seen: 0\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tExamples seen: 0\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tlearning_rate: 0.01\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tmonitor_seconds_per_epoch: 0.0\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\ttrain_recon_error: 259.559506353\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\ttrain_term_1: 0.221905011971\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Time this epoch: 0.000000 seconds\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Monitoring step:\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tEpochs seen: 1\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tBatches seen: 25\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tExamples seen: 500\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tlearning_rate: 0.01\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tmonitor_seconds_per_epoch: 0.0\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\ttrain_recon_error: 78.6476149351\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\ttrain_term_1: 0.0432361460984\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Time this epoch: 0.000000 seconds\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Monitoring step:\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tEpochs seen: 2\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tBatches seen: 50\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tExamples seen: 1000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tlearning_rate: 0.01\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tmonitor_seconds_per_epoch: 0.0\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\ttrain_recon_error: 74.6738205558\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\ttrain_term_1: 0.0270374776463\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Time this epoch: 0.000000 seconds\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Monitoring step:\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tEpochs seen: 3\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tBatches seen: 75\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tExamples seen: 1500\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tlearning_rate: 0.01\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tmonitor_seconds_per_epoch: 0.0\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\ttrain_recon_error: 71.8095027129\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\ttrain_term_1: 0.0215087557302\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Time this epoch: 0.000000 seconds\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Monitoring step:\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tEpochs seen: 4\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tBatches seen: 100\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tExamples seen: 2000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tlearning_rate: 0.01\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tmonitor_seconds_per_epoch: 0.0\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\ttrain_recon_error: 68.1916838912\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\ttrain_term_1: 0.0182252056256\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Time this epoch: 0.000000 seconds\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Monitoring step:\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tEpochs seen: 5\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tBatches seen: 125\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tExamples seen: 2500\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tlearning_rate: 0.01\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tmonitor_seconds_per_epoch: 0.0\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\ttrain_recon_error: 66.0900067852\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\ttrain_term_1: 0.0165280992906\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Time this epoch: 0.000000 seconds\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Monitoring step:\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tEpochs seen: 6\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tBatches seen: 150\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tExamples seen: 3000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tlearning_rate: 0.01\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tmonitor_seconds_per_epoch: 0.0\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\ttrain_recon_error: 63.2291542366\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\ttrain_term_1: 0.0154138852973\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Time this epoch: 0.000000 seconds\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Monitoring step:\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tEpochs seen: 7\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tBatches seen: 175\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tExamples seen: 3500\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tlearning_rate: 0.01\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tmonitor_seconds_per_epoch: 0.0\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\ttrain_recon_error: 61.1517331954\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\ttrain_term_1: 0.014698918568\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Time this epoch: 0.000000 seconds\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Monitoring step:\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tEpochs seen: 8\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tBatches seen: 200\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tExamples seen: 4000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tlearning_rate: 0.01\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tmonitor_seconds_per_epoch: 0.0\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\ttrain_recon_error: 58.6003009301\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\ttrain_term_1: 0.0134196279523\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Time this epoch: 0.000000 seconds\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Monitoring step:\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tEpochs seen: 9\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tBatches seen: 225\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tExamples seen: 4500\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tlearning_rate: 0.01\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tmonitor_seconds_per_epoch: 0.0\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\ttrain_recon_error: 56.4969002425\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\ttrain_term_1: 0.0126134828104\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Time this epoch: 0.000000 seconds\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Monitoring step:\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tEpochs seen: 10\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tBatches seen: 250\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tExamples seen: 5000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tlearning_rate: 0.01\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tmonitor_seconds_per_epoch: 0.0\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\ttrain_recon_error: 54.5938713402\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\ttrain_term_1: 0.0121323184144\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Saving to my_rbm_trainsave_1021.pkl...\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Saving to my_rbm_trainsave_1021.pkl done. Time elapsed: 0.000000 seconds\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Time this epoch: 0.000000 seconds\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Monitoring step:\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tEpochs seen: 11\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tBatches seen: 275\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tExamples seen: 5500\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tlearning_rate: 0.01\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tmonitor_seconds_per_epoch: 0.0\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\ttrain_recon_error: 52.7161228876\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\ttrain_term_1: 0.0115466310676\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Time this epoch: 0.000000 seconds\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Monitoring step:\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tEpochs seen: 12\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tBatches seen: 300\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tExamples seen: 6000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tlearning_rate: 0.01\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tmonitor_seconds_per_epoch: 0.0\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\ttrain_recon_error: 51.0871767715\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\ttrain_term_1: 0.0108321831679\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Time this epoch: 0.000000 seconds\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Monitoring step:\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tEpochs seen: 13\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tBatches seen: 325\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tExamples seen: 6500\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tlearning_rate: 0.01\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tmonitor_seconds_per_epoch: 0.0\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\ttrain_recon_error: 49.4790349768\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\ttrain_term_1: 0.0103293937741\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Time this epoch: 0.000000 seconds\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Monitoring step:\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tEpochs seen: 14\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tBatches seen: 350\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tExamples seen: 7000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tlearning_rate: 0.01\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tmonitor_seconds_per_epoch: 0.0\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\ttrain_recon_error: 48.1181708843\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\ttrain_term_1: 0.0100181965842\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Time this epoch: 0.000000 seconds\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Monitoring step:\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tEpochs seen: 15\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tBatches seen: 375\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tExamples seen: 7500\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tlearning_rate: 0.01\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tmonitor_seconds_per_epoch: 0.0\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\ttrain_recon_error: 46.9436324556\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\ttrain_term_1: 0.00954804583851\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Saving to my_rbm_trainsave_1021.pkl...\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Saving to my_rbm_trainsave_1021.pkl done. Time elapsed: 0.000000 seconds\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "time elapsed on training is 17.12\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "for\u5faa\u73af\u7248\u7684CD\u8bad\u7ec3\u7b97\u6cd5"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\u65e7\u7248\u9057\u7559\u7684\u4e24\u4e2a\u57fa\u4e8efor\u5faa\u73af\u7684Gibbs\u62bd\u6837\u51fd\u6570\uff0c\u6216\u8bb8\u5bf9\u4e8e\u5982\u4f55\u5728\u7b26\u53f7\u8ba1\u7b97\u4e2d\u4f7f\u7528for\u5faa\u73af\uff0c\u4eca\u540e\u6709\u53c2\u8003\u4ef7\u503c"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "class MyCD_for(MyContrastiveDivergence):\n",
      "    \"\"\"deprecated, use scan version instead\"\"\"\n",
      "    def get_gradients(self, model, data, ** kwargs):\n",
      "        #print 'get_gradients'\n",
      "        pos_v = data\n",
      "        #chain_start = pos_v\n",
      "        v_samples = pos_v\n",
      "        #print 'v_samples', v_samples.ndim\n",
      "        for i in xrange(self.k):\n",
      "            v_samples = model.gibbs_vhv(v_samples)[-1]\n",
      "        #[act_hids, hid_mfs, hid_samples, act_vis, vis_mfs, vis_samples], updates = theano.scan(fn = model.gibbs_vhv, sequences=None, \n",
      "\t\t                        #outputs_info=[None, None, None, None, None, chain_start], non_sequences=None, n_steps=self.k)\n",
      "        neg_v = v_samples\n",
      "        \n",
      "        cost = -(- model.free_energy(pos_v).mean() + model.free_energy(neg_v).mean())\n",
      "\n",
      "        params = list(model.get_params())\n",
      "\n",
      "        grads = T.grad(cost, params, disconnected_inputs = 'ignore', consider_constant=[pos_v, neg_v])\n",
      "\n",
      "        gradients = OrderedDict(izip(params, grads))\n",
      "\n",
      "        updates = OrderedDict()\n",
      "\n",
      "        return gradients, updates\n",
      "    \n",
      "class MyPCD_for(MyContrastiveDivergence):\n",
      "    \"\"\"deprecated, use scan version instead\"\"\"\n",
      "    def get_gradients(self, model, data, ** kwargs):\n",
      "        #print 'get_gradients'\n",
      "        chain_start = theano.shared(numpy.zeros(shape=(self.chain_num, model.n_vis)), name=None, borrow=True)\n",
      "        v_samples = chain_start\n",
      "        \n",
      "        for i in xrange(self.k):\n",
      "            v_samples = model.gibbs_vhv(v_samples)[-1]\n",
      "        chain_end = v_samples\n",
      "        #print 'chain_end', chain_end.ndim\n",
      "        chain_updates = {}\n",
      "        chain_updates[chain_start] = chain_end\n",
      "        \n",
      "        pos_v = data\n",
      "        #neg_v = self.get_neg_v(model)\n",
      "        \n",
      "        cost = -(- model.free_energy(pos_v).mean() + model.free_energy(chain_end).mean())\n",
      "\n",
      "        params = list(model.get_params())\n",
      "\n",
      "        grads = T.grad(cost, params, disconnected_inputs = 'ignore', consider_constant=[chain_end])\n",
      "\n",
      "        gradients = OrderedDict(izip(params, grads))\n",
      "\n",
      "        updates = OrderedDict()\n",
      "        \n",
      "        updates.update(chain_updates) # manual added\n",
      "\n",
      "        return gradients, updates\n",
      "\n",
      "class MyCD_scan(MyContrastiveDivergence):\n",
      "    \n",
      "    def get_gradients(self, model, data, ** kwargs):\n",
      "        #print 'get_gradients'\n",
      "        pos_v = data\n",
      "        #chain_start = pos_v\n",
      "        v_samples = pos_v\n",
      "        #print 'v_samples', v_samples.ndim\n",
      "        [act_hids, hid_mfs, hid_samples, act_vis, vis_mfs, vis_samples], scan_updates = theano.scan(fn = model.gibbs_vhv, sequences=None, \n",
      "\t\t                        outputs_info=[None, None, None, None, None, v_samples], non_sequences=None, n_steps=self.k)\n",
      "        neg_v = vis_samples[-1]\n",
      "        \n",
      "        cost = -(- model.free_energy(pos_v).mean() + model.free_energy(neg_v).mean())\n",
      "\n",
      "        params = list(model.get_params())\n",
      "\n",
      "        grads = T.grad(cost, params, disconnected_inputs = 'ignore', consider_constant=[pos_v, neg_v])\n",
      "\n",
      "        gradients = OrderedDict(izip(params, grads))\n",
      "\n",
      "        updates = OrderedDict()\n",
      "        \n",
      "        updates.update(scan_updates) # add scan_updates\n",
      "\n",
      "        return gradients, updates"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'MyContrastiveDivergence' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-1-8001bfa9ea4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mMyCD_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMyContrastiveDivergence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;34m\"\"\"deprecated, use scan version instead\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m#print 'get_gradients'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mNameError\u001b[0m: name 'MyContrastiveDivergence' is not defined"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "PCD\u7b97\u6cd5(\u7591\u4f3c\u6709\u8bef)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#\u6000\u7591\u8be5\u65b9\u6cd5\u6709\u8bef\uff0c\u5e94\u91cd\u5199\u4e00\u4e2anumpy\u7248\uff0c\u8fdb\u884c\u5bf9\u6bd4\n",
      "class MyPCD_scan(MyContrastiveDivergence):\n",
      "    \n",
      "    def get_gradients(self, model, data, ** kwargs):\n",
      "        #print 'get_gradients'\n",
      "        chain_start = theano.shared(numpy.zeros(shape=(self.chain_num, model.n_vis), dtype=theano.config.floatX), name='chain_start', borrow=True)\n",
      "        \n",
      "        [h_mean, h_sample, v_mean, v_sample], scan_updates = theano.scan(fn = model.gibbs_vhv, sequences=None, \n",
      "\t\t                        outputs_info=[None, None, None, chain_start], non_sequences=None, n_steps=self.k)\n",
      "    \n",
      "        chain_end = v_sample[-1]\n",
      "        scan_updates[chain_start] = chain_end\n",
      "        \n",
      "        pos_v = data \n",
      "        \n",
      "        cost = -(- model.free_energy(pos_v).mean() + model.free_energy(chain_end).mean())\n",
      "\n",
      "        params = list(model.get_params())\n",
      "\n",
      "        grads = T.grad(cost, params, disconnected_inputs = 'ignore', consider_constant=[pos_v, chain_end])\n",
      "\n",
      "        gradients = OrderedDict(izip(params, grads))\n",
      "\n",
      "        updates = OrderedDict()\n",
      "        \n",
      "        updates.update(scan_updates) # manual added\n",
      "\n",
      "        return gradients, updates"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "\u65e7\u7248RBM"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\u65e7\u7248\u7684binary-binary RBM\uff0c\u7279\u70b9\u662fsample_v_given_h()\u548csample_h_given_v()\u90fd\u8fd4\u56de\u4e09\u4e2a\u503c\uff0c\u800cgibbs_vhv()\u548cgibbs_hvh()\u8fd4\u56de6\u4e2a\u503c\n",
      "\u4e8b\u5b9e\u8bc1\u660e\uff0cgibbs\u51fd\u6570\u8fd4\u56de4\u4e2a\u503c\u662f\u6bd4\u8f83\u7406\u60f3\u7684\u4e00\u79cd\u9009\u62e9"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import time\n",
      "import os\n",
      "from itertools import izip\n",
      "import copy\n",
      "\n",
      "import numpy\n",
      "import theano\n",
      "import theano.tensor as T\n",
      "\n",
      "from theano.tensor.shared_randomstreams import RandomStreams\n",
      "from theano.sandbox.rng_mrg import MRG_RandomStreams\n",
      "from theano.compat.python2x import OrderedDict\n",
      "\n",
      "from pylearn2.base import Block\n",
      "from pylearn2.models.model import Model\n",
      "from pylearn2.linear.matrixmul import MatrixMul\n",
      "from pylearn2.costs.cost import Cost\n",
      "from pylearn2.models.mlp import MLP, Layer\n",
      "from pylearn2.datasets.mnist import MNIST\n",
      "from pylearn2.space import VectorSpace\n",
      "from pylearn2.costs.cost import SumOfCosts\n",
      "from pylearn2.utils import sharedX, as_floatX\n",
      "\n",
      "from module.dataset_from_design import DatasetFromDesign\n",
      "\n",
      "\n",
      "theano.config.compute_test_value = 'off'\n",
      "\n",
      "class HonglakLeeSparse(Cost):\n",
      "    \n",
      "    def __init__(self, p=0.02):\n",
      "        self.p = p\n",
      "    \n",
      "    def expr(self, model, data):\n",
      "        \n",
      "        v = data\n",
      "        p_h_given_v_matrix = model.propup(v)[-1]\n",
      "        sum_meta = T.square(self.p - T.mean(p_h_given_v_matrix, axis=0, dtype=theano.config.floatX))\n",
      "        expr = T.sum(sum_meta)\n",
      "        \n",
      "        return expr\n",
      "    \n",
      "    def get_data_specs(self, model):\n",
      "        return (model.get_input_space(), model.get_input_source())\n",
      "\n",
      "class MyContrastiveDivergence(Cost):\n",
      "    \n",
      "    def __init__(self, k, chain_num=None): # CD-k\n",
      "        # k: CD-k\n",
      "        self.k = k\n",
      "        self.chain_num = chain_num\n",
      "\t\n",
      "    def expr(self, model, data):\n",
      "        #pos_v = data\n",
      "        #v_samples = pos_v\n",
      "        #for i in xrange(self.k):\n",
      "        #    v_samples = model.gibbs_vhv(v_samples)[-1]\n",
      "        #neg_v = v_samples\n",
      "        #neg_log_likelihood = -(- model.free_energy(pos_v).mean() + model.free_energy(neg_v).mean())\n",
      "        #return neg_log_likelihood\n",
      "\t    return None\n",
      "        \n",
      "    def get_data_specs(self, model):\n",
      "        return model.get_monitoring_data_specs()\n",
      "\n",
      "    \n",
      "class MyCD_scan(MyContrastiveDivergence):\n",
      "    \n",
      "    def get_gradients(self, model, data, ** kwargs):\n",
      "        #print 'get_gradients'\n",
      "        pos_v = data\n",
      "        #chain_start = pos_v\n",
      "        v_samples = pos_v\n",
      "        #print 'v_samples', v_samples.ndim\n",
      "        [act_hids, hid_mfs, hid_samples, act_vis, vis_mfs, vis_samples], scan_updates = theano.scan(fn = model.gibbs_vhv, sequences=None, \n",
      "\t\t                        outputs_info=[None, None, None, None, None, v_samples], non_sequences=None, n_steps=self.k)\n",
      "        neg_v = vis_samples[-1]\n",
      "        \n",
      "        cost = -(- model.free_energy(pos_v).mean() + model.free_energy(neg_v).mean())\n",
      "\n",
      "        params = list(model.get_params())\n",
      "\n",
      "        grads = T.grad(cost, params, disconnected_inputs = 'ignore', consider_constant=[pos_v, neg_v])\n",
      "\n",
      "        gradients = OrderedDict(izip(params, grads))\n",
      "\n",
      "        updates = OrderedDict()\n",
      "        \n",
      "        updates.update(scan_updates) # add scan_updates\n",
      "\n",
      "        return gradients, updates\n",
      "\n",
      "class MyCD_energy_scan(MyContrastiveDivergence):\n",
      "    \n",
      "    def get_gradients(self, model, data, ** kwargs):\n",
      "        #print 'get_gradients'\n",
      "        pos_v = data\n",
      "        pos_h = model.sample_h_given_v(pos_v)[-1]\n",
      "        #chain_start = pos_v\n",
      "        h_samples = pos_h\n",
      "        #print 'v_samples', v_samples.ndim\n",
      "        [act_vis, vis_mfs, vis_samples, act_hids, hid_mfs, hid_samples], scan_updates = theano.scan(fn = model.gibbs_hvh, sequences=None, \n",
      "\t\t                        outputs_info=[None, None, None, None, None, h_samples], non_sequences=None, n_steps=self.k)\n",
      "        neg_v = vis_samples[-1]\n",
      "        neg_h = hid_samples[-1]\n",
      "        \n",
      "        cost = -(- model.energy(pos_v, pos_h).mean() + model.energy(neg_v, neg_h).mean())\n",
      "\n",
      "        params = list(model.get_params())\n",
      "\n",
      "        grads = T.grad(cost, params, disconnected_inputs = 'ignore', consider_constant=[pos_v, pos_h, neg_v, neg_h])\n",
      "\n",
      "        gradients = OrderedDict(izip(params, grads))\n",
      "\n",
      "        updates = OrderedDict()\n",
      "        \n",
      "        updates.update(scan_updates) # add scan_updates\n",
      "\n",
      "        return gradients, updates\n",
      "    \n",
      "class MyCD_free_energy_scan(MyContrastiveDivergence):\n",
      "    \n",
      "    def get_gradients(self, model, data, ** kwargs):\n",
      "        #print 'get_gradients'\n",
      "        pos_v = data\n",
      "        #pos_h = model.sample_h_given_v(pos_v)[-1]\n",
      "        #chain_start = pos_v\n",
      "        #h_samples = pos_h\n",
      "        #print 'v_samples', v_samples.ndim\n",
      "        [act_hids, hid_mfs, hid_samples, act_vis, vis_mfs, vis_samples], scan_updates = theano.scan(fn = model.gibbs_vhv, sequences=None, \n",
      "\t\t                        outputs_info=[None, None, None, pos_v], non_sequences=None, n_steps=self.k)\n",
      "        neg_v = vis_samples[-1]\n",
      "        #neg_h = hid_samples[-1]\n",
      "        \n",
      "        cost = -(- model.free_energy(pos_v).mean() + model.free_energy(neg_v).mean())\n",
      "\n",
      "        params = list(model.get_params())\n",
      "\n",
      "        grads = T.grad(cost, params, disconnected_inputs = 'ignore', consider_constant=[pos_v, neg_v])\n",
      "\n",
      "        gradients = OrderedDict(izip(params, grads))\n",
      "\n",
      "        updates = OrderedDict()\n",
      "        \n",
      "        updates.update(scan_updates) # add scan_updates\n",
      "\n",
      "        return gradients, updates\n",
      "\n",
      "\t\n",
      "class MyPCD_scan(MyContrastiveDivergence):\n",
      "    \n",
      "    def get_gradients(self, model, data, ** kwargs):\n",
      "        #print 'get_gradients'\n",
      "        chain_start = theano.shared(numpy.zeros(shape=(self.chain_num, model.n_vis), dtype=theano.config.floatX), name='chain_start', borrow=True)\n",
      "        \n",
      "        [act_hids, hid_mfs, hid_samples, act_vis, vis_mfs, vis_samples], scan_updates = theano.scan(fn = model.gibbs_vhv, sequences=None, \n",
      "\t\t                        outputs_info=[None, None, None, None, None, chain_start], non_sequences=None, n_steps=self.k)\n",
      "    \n",
      "        chain_end = vis_samples[-1]\n",
      "        scan_updates[chain_start] = chain_end\n",
      "        \n",
      "        pos_v = data \n",
      "        \n",
      "        cost = -(- model.free_energy(pos_v).mean() + model.free_energy(chain_end).mean())\n",
      "\n",
      "        params = list(model.get_params())\n",
      "\n",
      "        grads = T.grad(cost, params, disconnected_inputs = 'ignore', consider_constant=[pos_v, chain_end])\n",
      "\n",
      "        gradients = OrderedDict(izip(params, grads))\n",
      "\n",
      "        updates = OrderedDict()\n",
      "        \n",
      "        updates.update(scan_updates) # manual added\n",
      "\n",
      "        return gradients, updates\n",
      "\t\n",
      "# Is that necessary to inherit Layer class??\n",
      "class MyRBM(Model, Block):\n",
      "    \"\"\"Restricted Boltzmann Machine (RBM)  \"\"\"\n",
      "    def __init__(self, n_vis, n_hid, W=None, h_bias=None, v_bias=None, numpy_rng=None,theano_rng=None):\n",
      "        Model.__init__(self) # self.names_to_del = set(); self._test_batch_size = 2\n",
      "        Block.__init__(self) # self.fn = None; self.cpu_only = False\n",
      "\n",
      "        self.n_vis = n_vis\n",
      "        self.n_hid = n_hid\n",
      "        \n",
      "        self.input_space = VectorSpace(dim=self.n_vis) # add input_space\n",
      "        self.output_space = VectorSpace(dim=self.n_hid) # add output_space\n",
      "\n",
      "        if numpy_rng is None:\n",
      "            # create a number generator\n",
      "            numpy_rng = numpy.random.RandomState(seed=19900418)\n",
      "        self.numpy_rng = numpy_rng\n",
      "\n",
      "        if theano_rng is None:\n",
      "            theano_rng = RandomStreams(numpy_rng.randint(2 ** 30))\n",
      "        self.theano_rng = theano_rng\n",
      "\n",
      "        if W is None:\n",
      "            init_W = numpy.asarray(numpy_rng.uniform(\n",
      "                      low=-4 * numpy.sqrt(6. / (n_hid + n_vis)),\n",
      "                      high=4 * numpy.sqrt(6. / (n_hid + n_vis)),\n",
      "                      size=(n_vis, n_hid)),\n",
      "                      dtype=theano.config.floatX)\n",
      "            # theano shared variables for weights and biases\n",
      "            W = theano.shared(value=init_W, name='W', borrow=True)\n",
      "\n",
      "        if h_bias is None:\n",
      "            # create shared variable for hidden units bias\n",
      "            h_bias = theano.shared(value=numpy.zeros(n_hid, dtype=theano.config.floatX), name='h_bias', borrow=True)\n",
      "\n",
      "        if v_bias is None:\n",
      "            # create shared variable for visible units bias\n",
      "            v_bias = theano.shared(value=numpy.zeros(n_vis, dtype=theano.config.floatX), name='v_bias', borrow=True)\n",
      "\n",
      "        self.W = W\n",
      "        self.h_bias = h_bias\n",
      "        self.v_bias = v_bias\n",
      "\n",
      "        self._params = [self.W, self.h_bias, self.v_bias]\n",
      "        \n",
      "    def get_monitoring_data_specs(self):\n",
      "\t    return (self.get_input_space(), self.get_input_source())\n",
      "\t\t\n",
      "    def get_monitoring_channels(self, data):\n",
      "        v = data\n",
      "        #H = self.sample_h_given_v(v)[1]\n",
      "        #h = H.mean(axis=0)\n",
      "        channels = {}\n",
      "\n",
      "        #channels =  { 'bias_hid_min' : T.min(self.h_bias),\n",
      "        #         'bias_hid_mean' : T.mean(self.h_bias),\n",
      "        #         'bias_hid_max' : T.max(self.h_bias),\n",
      "        #         'bias_vis_min' : T.min(self.v_bias),\n",
      "        #         'bias_vis_mean' : T.mean(self.v_bias),\n",
      "        #         'bias_vis_max': T.max(self.v_bias),\n",
      "        #         'h_min' : T.min(h),\n",
      "        #         'h_mean': T.mean(h),\n",
      "        #         'h_max' : T.max(h),\n",
      "                 #'W_min' : T.min(self.weights),\n",
      "                 #'W_max' : T.max(self.weights),\n",
      "                 #'W_norms_min' : T.min(norms),\n",
      "                 #'W_norms_max' : T.max(norms),\n",
      "                 #'W_norms_mean' : T.mean(norms),\n",
      "        #}\n",
      "\t\t# recon_error\n",
      "        channel_name = 'recon_error'\n",
      "        p_v_given_h, v_sample = self.gibbs_vhv(v)[4:]\n",
      "        recon_error = ((p_v_given_h - v) ** 2).mean(axis=0).sum()\n",
      "        channels[channel_name] = recon_error\n",
      "        \n",
      "        #pos_v = data\n",
      "        #[h_act, h_mean, h_sample, v_act, v_mean, v_sample], scan_updates = theano.scan(fn = self.gibbs_vhv, sequences=None, \n",
      "\t\t#                        outputs_info=[None, None, None, None, None, pos_v], non_sequences=None, n_steps=1)\n",
      "        #pos_h = h_sample[0]\n",
      "        #neg_v = v_sample[-1]\n",
      "        #neg_h = self.sample_h_given_v(v_sample[-1])[-1]\n",
      "        #cost = -(- self.energy(pos_v, pos_h).mean() + self.energy(neg_v, neg_h).mean())\n",
      "        #channels['energy_cost'] = cost\n",
      "        \n",
      "        #chain_start = theano.shared(numpy.zeros(shape=(20, self.n_vis), dtype=theano.config.floatX), name='chain_start', borrow=True)\n",
      "        #[h_act, h_mean, h_sample, v_act, v_mean, v_sample], scan_updates = theano.scan(fn = self.gibbs_vhv, sequences=None, \n",
      "\t\t#                        outputs_info=[None, None, None, None, None, chain_start], non_sequences=None, n_steps=1)\n",
      "        #chain_end = v_sample[-1]\n",
      "        #scan_updates[chain_start] = chain_end\n",
      "        #pos_v = data \n",
      "        #cost = -(- self.free_energy(pos_v).mean() + self.free_energy(chain_end).mean())\n",
      "        #channels['free_enegy_cost'] = cost\n",
      "        \n",
      "\t\t#pseudo_likelihood\n",
      "        #channel_name = 'pseudo_likelihood'\n",
      "        #bit_i_idx = theano.shared(value=0, name='bit_i_idx')\n",
      "        #xi = T.round(v)\n",
      "        #print 'xi',xi.ndim\n",
      "        #fe_xi = self.free_energy(xi)\n",
      "        #xi_flip = T.set_subtensor(xi[:, bit_i_idx], 1 - xi[:, bit_i_idx])\n",
      "        #print 'xi_flip', xi_flip.ndim\n",
      "        #fe_xi_flip = self.free_energy(xi_flip)\n",
      "        #cost = T.mean(self.n_vis * T.log(T.nnet.sigmoid(fe_xi_flip - fe_xi)))\n",
      "        #updates[bit_i_idx] = (bit_i_idx + 1) % self.n_vis\n",
      "        #channels[channel_name] = cost\n",
      "        \n",
      "        return channels\n",
      "    \n",
      "    def energy(self, v, h):\n",
      "        W, c, b = self.get_params()\n",
      "        #energy = - (T.dot(v, b) + T.dot(h, c) + T.dot(T.dot(v, W), h.T) * T.eye(n=v.shape[0], m=h.shape[0]).sum(axis=0))\n",
      "        energy = - (T.dot(v, b) + T.dot(h, c) + (T.dot(v, W) * h).sum(axis=1))\n",
      "        #energy = - (T.dot(v, b) + T.dot(h, c) + T.dot((T.dot(v, W)).T, h))\n",
      "        return energy\n",
      "\n",
      "    def free_energy(self, v_sample):\n",
      "        #print 'free_energy'\n",
      "        ''' Function to compute the free energy '''\n",
      "        wx_b = T.dot(v_sample, self.W) + self.h_bias\n",
      "        #print 'wx_b', wx_b.ndim\n",
      "        v_bias_term = T.dot(v_sample, self.v_bias)\n",
      "        softplus_term = T.sum(T.nnet.softplus(wx_b), axis=1)\n",
      "        return - v_bias_term - softplus_term\n",
      "\n",
      "    def propup(self, vis):\n",
      "        pre_sigmoid_activation = T.dot(vis, self.W) + self.h_bias\n",
      "        return [pre_sigmoid_activation, T.nnet.sigmoid(pre_sigmoid_activation)]\n",
      "\n",
      "    def sample_h_given_v(self, v0_sample):\n",
      "        ''' This function infers state of hidden units given visible units '''\n",
      "        # compute the activation of the hidden units given a sample of\n",
      "        # the visibles\n",
      "        pre_sigmoid_h1, h1_mean = self.propup(v0_sample)\n",
      "        h1_sample = self.theano_rng.binomial(size=h1_mean.shape,\n",
      "                                             n=1, p=h1_mean,\n",
      "                                             dtype=theano.config.floatX)\n",
      "        return [pre_sigmoid_h1, h1_mean, h1_sample]\n",
      "\n",
      "    def propdown(self, hid):\n",
      "        pre_sigmoid_activation = T.dot(hid, self.W.T) + self.v_bias\n",
      "        return [pre_sigmoid_activation, T.nnet.sigmoid(pre_sigmoid_activation)]\n",
      "\n",
      "    def sample_v_given_h(self, h0_sample):\n",
      "        ''' This function infers state of visible units given hidden units '''\n",
      "        # compute the activation of the visible given the hidden sample\n",
      "        pre_sigmoid_v1, v1_mean = self.propdown(h0_sample)\n",
      "        v1_sample = self.theano_rng.binomial(size=v1_mean.shape,\n",
      "                                             n=1, p=v1_mean,\n",
      "                                             dtype=theano.config.floatX)\n",
      "        return [pre_sigmoid_v1, v1_mean, v1_sample]\n",
      "\n",
      "    def gibbs_hvh(self, h0_sample):\n",
      "        ''' This function implements one step of Gibbs sampling,\n",
      "            starting from the hidden state'''\n",
      "        pre_sigmoid_v1, v1_mean, v1_sample = self.sample_v_given_h(h0_sample)\n",
      "        pre_sigmoid_h1, h1_mean, h1_sample = self.sample_h_given_v(v1_sample)\n",
      "        return [pre_sigmoid_v1, v1_mean, v1_sample,\n",
      "                pre_sigmoid_h1, h1_mean, h1_sample]\n",
      "\n",
      "    def gibbs_vhv(self, v0_sample):\n",
      "        ''' This function implements one step of Gibbs sampling,\n",
      "            starting from the visible state'''\n",
      "        pre_sigmoid_h1, h1_mean, h1_sample = self.sample_h_given_v(v0_sample)\n",
      "        pre_sigmoid_v1, v1_mean, v1_sample = self.sample_v_given_h(h1_sample)\n",
      "        return [pre_sigmoid_h1, h1_mean, h1_sample, pre_sigmoid_v1, v1_mean, v1_sample]\n",
      "\t\t\t\t\n",
      "\t# interface for pylearn2.model.mlp PretraindLayer\n",
      "    def upward_pass(self, state_below):\n",
      "        return self.propup(state_below)[1]\n",
      "    \n",
      "    def visual_config(self, step=1, dpi=240, start=0, total_num=100, v_shape=(28, 28), v_channels=1, random=False):\n",
      "        assert total_num < self.n_hid - start\n",
      "        self.__dict__.update(locals())\n",
      "        del self.self\n",
      "    \n",
      "    #need to be cooperate with module.my_train_extensions.Visualizer(TrainExtension) instance\n",
      "    # which shoud be added to extensions prameter of pylearn2..train.Train\n",
      "    def get_visual(self):\n",
      "        #step: draw pictures every step epochs\n",
      "        #total_num: number of hidden units which need to be draw\n",
      "        #v_shape: shape of input image\n",
      "        #v_channels: number of channels that each input has\n",
      "        # random: if True, randomly select total_num hiddenunits, else use the first total_num hidden units\n",
      "        assert total_num < self.n_hid\n",
      "        save_path = os.getcwd() + '/visual_pic'\n",
      "        if not os.path.isdir(save_path):\n",
      "            os.mkdir(save_path)\n",
      "        if hasattr(self, 'epoch_cnt'):\n",
      "            self.epoch_cnt += 1\n",
      "        else:\n",
      "            self.epoch_cnt = 1\n",
      "        rng = self.numpy_rng\n",
      "        W = self.W.get_value()\n",
      "        v = W / ((W**2).sum(axis=1)).reshape(W.shape[0], 1)\n",
      "        \n",
      "        if (self.epoch_cnt - 1) % self.step != 0:\n",
      "            return None\n",
      "        \n",
      "        if self.random is True:\n",
      "            rl = rng.choice(xrange(self.n_hid), self.total_num)\n",
      "        else:\n",
      "            rl = range(self.total_num)\n",
      "        \n",
      "        l = int(numpy.ceil(numpy.sqrt(self.total_num)))\n",
      "\n",
      "        for i in xrange(l):\n",
      "            for j in xrange(l):\n",
      "                if self.v_channels == 1: #draw gray level img\n",
      "                    pylab.subplot(l, l, i*l+j+1); pylab.axis('off')\n",
      "                    pylab.imshow(v[:, rl[i*l+j]].reshape(self.v_shape), cmap=pylab.cm.gray)\n",
      "                else: # draw RGB img\n",
      "                    pylab.subplot(l, l, i*l+j+1); pylab.axis('off')\n",
      "                    pylab.imshow(v[:, rl[i*l+j]].reshape(3, self.v_shape[0], self.v_shape[1]).transpose(1,2,0))\n",
      "\n",
      "        pylab.savefig(save_path + '/epoch_' + str(self.epoch_cnt) + '.png', dpi=self.dpi) # save img\n",
      "        \n",
      "        return None\n",
      "        \n",
      "    # default cost is cd-1\n",
      "    def get_default_cost(self):\n",
      "        return MyCD_free_energy_scan(k=1)\n",
      "    \n",
      "    def make_dataset(self, dataset,sample=False): # use rbm as a feature extractor, daatset pass through the filter and produce a new datset\n",
      "        \n",
      "        orin = T.matrix()\n",
      "        if sample == False:\n",
      "            f = theano.function([orin], self.propup(orin)[-1])\n",
      "        else:\n",
      "            f = theano.function([orin], self.sample_h_given_v(orin)[-1])\n",
      "        X_new = f(dataset.X)\n",
      "        new_ds = DatasetFromDesign(design_matrix=X_new, label=dataset.y)\n",
      "        #print new_ds.__dict__\n",
      "        #print X_new.shape\n",
      "        return new_ds\n",
      "    \n",
      "if __name__ == '__main__':\n",
      "    from pylearn2.datasets.mnist import MNIST\n",
      "    from pylearn2.training_algorithms.sgd import SGD\n",
      "    from pylearn2.train import Train\n",
      "    from pylearn2.termination_criteria import MonitorBased\n",
      "    from pylearn2.train_extensions.best_params import MonitorBasedSaveBest\n",
      "    from pylearn2.training_algorithms.sgd import MonitorBasedLRAdjuster\n",
      "    from pylearn2.training_algorithms.sgd import MomentumAdjustor\n",
      "    from pylearn2.termination_criteria import EpochCounter\n",
      "    from pylearn2.costs.cost import SumOfCosts\n",
      "\t\n",
      "\t\n",
      "    dsm_train = MNIST(which_set='train', start=0, stop=500, one_hot=True)\n",
      "    #dsm_valid = MNIST(which_set='train', start=500, stop=600, one_hot=True)\n",
      "    #dsm_test = MNIST(which_set='test', start=0, stop=100, one_hot=True)\n",
      "\t\n",
      "    #monitoring_dataset = {'train': dsm_train, 'valid': dsm_valid, 'test': dsm_test}\n",
      "    monitoring_dataset = {'train': dsm_train}\n",
      "\t\n",
      "    rbm_model = MyRBM(n_vis=784, n_hid=500)\n",
      "    \n",
      "    #cd_cost = MyCD_for(k=15)\n",
      "    #cd_cost = MyCD_scan(k=15)\n",
      "    #cd_cost = MyPCD_for(k=15, chain_num=20)\n",
      "    total_cost = MyPCD_scan(k=15, chain_num=20)\n",
      "    \n",
      "    #total_cost = MyCD_energy_scan(k=1)\n",
      "    #total_cost = SumOfCosts(costs=[MyCD_energy_scan(k=1), (1/ 0.05, HonglakLeeSparse(p=0.05))])\n",
      "    \n",
      "    #alg = SGD(learning_rate=0.1, cost=total_cost, batch_size=20, init_momentum=None, monitoring_dataset=monitoring_dataset,\n",
      "              #termination_criterion=MonitorBased(channel_name='valid_recon_error', N=10))\n",
      "    #          termination_criterion=EpochCounter(max_epochs=100))\n",
      "    \n",
      "    alg = SGD(learning_rate=0.01, cost=None, batch_size=20, init_momentum=None, monitoring_dataset=monitoring_dataset,\n",
      "              #termination_criterion=MonitorBased(channel_name='valid_recon_error', N=10))\n",
      "              termination_criterion=EpochCounter(max_epochs=15))\n",
      "    \n",
      "    #MonitorBasedLRAdjuster(dataset_name='valid'),MomentumAdjustor(start=1, saturate=20, final_momentum=.99)\n",
      "    train = Train(dataset=dsm_train, model=rbm_model, algorithm=alg,\n",
      "            #extensions=[MonitorBasedSaveBest(channel_name='test_recon_error', save_path='my_rbm_1021.pkl')],\n",
      "            save_path='my_rbm_trainsave_1021.pkl',\n",
      "            save_freq=10)\n",
      "    t0 = time.clock()\n",
      "    train.main_loop()\n",
      "    t1 = time.clock()\n",
      "    print 'time elapsed on training is', t1 - t0\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}