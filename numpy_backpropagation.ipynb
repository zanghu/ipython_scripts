{
 "metadata": {
  "name": "numpy_backpropagation"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "wiki article\u6570\u636e\u96c6\u4e00\u5171\u670910\u7c7b\uff0c\u603b\u6837\u672c\u65702866\u4e2a\uff0c\u5206\u4e3a\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6\u4e24\u4e2a\u90e8\u5206\n",
      "\u8bad\u7ec3\u96c6\uff1a2173\uff0c\u6d4b\u8bd5\u96c6\uff1a693\n",
      "\u6bcf\u4e2a\u6837\u672c\u5305\u542b\u4e09\u4e2a\u90e8\u5206\uff1a\u56fe\u50cf\u7279\u5f81\uff0c\u6587\u672c\u8bcd\u5411\u91cf\uff0c\u7c7b\u6807\n",
      "\u56fe\u50cf\u7279\u5f81\uff1a2296\n",
      "\u8bcd\u5411\u91cf\u5b57\u5178\uff1a3000\n",
      "\u7c7b\u6807\uff1a\u975eone-hot"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy\n",
      "import theano\n",
      "import theano.tensor as T\n",
      "import time\n",
      "from itertools import izip\n",
      "#import cPickle\n",
      "\n",
      "#1.\u8bfb\u53d6\u6837\u672c\n",
      "f = open('/home/zanghu/Pro_Datasets/POTD/train_img_data.npy')\n",
      "img_train = numpy.load(f)\n",
      "f.close()\n",
      "\n",
      "f = open('/home/zanghu/Pro_Datasets/POTD/train_txt_data.npy')\n",
      "txt_train = numpy.load(f)\n",
      "f.close()\n",
      "\n",
      "f = open('/home/zanghu/Pro_Datasets/POTD/train_lab_data.npy')\n",
      "lab_train = numpy.load(f)\n",
      "f.close()\n",
      "\n",
      "f = open('/home/zanghu/Pro_Datasets/POTD/test_img_data.npy')\n",
      "img_test = numpy.load(f)\n",
      "f.close()\n",
      "\n",
      "f = open('/home/zanghu/Pro_Datasets/POTD/test_txt_data.npy')\n",
      "txt_test = numpy.load(f)\n",
      "f.close()\n",
      "\n",
      "f = open('/home/zanghu/Pro_Datasets/POTD/test_lab_data.npy')\n",
      "lab_test = numpy.load(f)\n",
      "f.close()\n",
      "\n",
      "name_list = ['img_train', 'txt_train', 'lab_train', 'img_test', 'txt_test', 'lab_test']\n",
      "l = [img_train, txt_train, lab_train, img_test, txt_test, lab_test]\n",
      "for name, i in izip(name_list, l):\n",
      "    print 'num samples of %s: ' % name, \n",
      "    print i.shape\n",
      "#2.\u786e\u5b9a\u6bcf\u4e00\u7c7b\u6709\u591a\u5c11\u6837\u672c\n",
      "l = [0 for i in xrange(10)]\n",
      "mark = -1\n",
      "for i in xrange(lab_train.shape[0]):\n",
      "    l[lab_train[i]] += 1\n",
      "    if mark < lab_train[i]:\n",
      "        mark = lab_train[i]\n",
      "print 'total classes: ', mark\n",
      "print 'samples per class: ', l"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "num samples of img_train:  (2173, 2296)\n",
        "num samples of txt_train:  (2173, 3000)\n",
        "num samples of lab_train:  (2173,)\n",
        "num samples of img_test:  (693, 2296)\n",
        "num samples of txt_test:  (693, 3000)\n",
        "num samples of lab_test:  (693,)\n",
        "total classes:  9\n",
        "samples per class:  [347, 138, 272, 244, 248, 202, 178, 186, 144, 214]\n"
       ]
      }
     ],
     "prompt_number": 108
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#3.\u6837\u672c\u5747\u503c\u4f55\u65b9\u5dee\n",
      "print img_train.mean()\n",
      "print numpy.std(img_train)\n",
      "print img_test.mean()\n",
      "print numpy.std(img_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "9.21823987613\n",
        "49.5017600861\n",
        "9.21457882011"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "53.9997263558\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy\n",
      "\n",
      "std_train = numpy.std(img_train, axis=0)\n",
      "mean_train = numpy.mean(img_train, axis=0)\n",
      "temp_img_train = (img_train - mean_train) / std_train\n",
      "\n",
      "f = open('/home/zanghu/img_train_meanstd.npy', 'w')\n",
      "numpy.save(f, temp_img_train)\n",
      "f.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "\u53cd\u5411\u4f20\u64ad\u7b97\u6cd5"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy\n",
      "import theano\n",
      "import theano.tensor as T\n",
      "from pylearn2.utils import sharedX, as_floatX\n",
      "\n",
      "import time\n",
      "import sys\n",
      "\n",
      "def sigmoid(h):\n",
      "    \"\"\"sigmoid\u51fd\u6570\"\"\"\n",
      "    x = T.matrix()\n",
      "    f = theano.function([x], T.nnet.sigmoid(x))\n",
      "    #return 1. / (1. + numpy.exp(-x))\n",
      "    return f(h)\n",
      "\n",
      "def d_sigmoid(x):\n",
      "    \"\"\"sigmoid\u51fd\u6570\u7684\u5bfc\u51fd\u6570\"\"\"\n",
      "    return sigmoid(x) * (1. - sigmoid(x))\n",
      "\n",
      "def softmax(wx_b):\n",
      "    \"\"\"\u8f93\u5165\u4e3a\u77e9\u9635\u7684softmax\u51fd\u6570\"\"\"\n",
      "    assert isinstance(wx_b, numpy.ndarray)\n",
      "    assert wx_b.ndim == 2\n",
      "    #max_vector = numpy.max(wx_b, axis=1)\n",
      "    #numerator = numpy.exp(wx_b - max_vector[:, numpy.newaxis])\n",
      "    #denominator = numerator.sum(axis=1)\n",
      "    #p_matrix = numerator / denominator[:, numpy.newaxis]\n",
      "    x = T.matrix()\n",
      "    f = theano.function([x], T.nnet.softmax(x))\n",
      "    #return p_matrix\n",
      "    return f(wx_b)\n",
      "\n",
      "def prosum(a, b):\n",
      "    \"\"\"\u4e24\u4e2amini-batch\u7684product-sum\"\"\"\n",
      "    print 'a class: %s, b class: %s' % (a.__class__.__name__, b.__class__.__name__)\n",
      "    assert isinstance(a, numpy.ndarray) and isinstance(b, numpy.ndarray)\n",
      "    assert a.ndim == 2 and b.ndim == 2\n",
      "    \n",
      "    return numpy.sum(a * b, axis=1) #\u8fd4\u56de\u503c\u662f\u4e00\u4e2a\u5411\u91cf\uff0c\u6bcf\u4e2a\u5206\u91cf\u4ee3\u8868a\u548cb\u5bf9\u5e94\u4f4d\u7f6e\u5411\u91cfproduct-sum\u7684\u8fd0\u7b97\u7ed3\u679c\n",
      "\n",
      "class NN(object):\n",
      "\n",
      "    def __init__(self, n_vis, layers, numpy_rng=None, irange=0.1):\n",
      "        \"\"\"\u6784\u9020\u51fd\u6570\"\"\"\n",
      "        self.n_vis = n_vis\n",
      "        #self.cost = cost\n",
      "        self.irange = irange\n",
      "        self.num_layers = len(layers)\n",
      "        if numpy_rng is None:\n",
      "            numpy_rng = numpy.random.RandomState(seed=19901225)\n",
      "        self.numpy_rng = numpy_rng\n",
      "        assert all([isinstance(layer, Layer) for layer in layers])\n",
      "        assert hasattr(layers[-1], 'get_last_delta')\n",
      "        self.layers = layers\n",
      "        #h[i]\u4ee3\u8868\u7d22\u5f15i\u5c42\u7684\u8f93\u5165\uff0cs[i]\u4ee3\u8868\u7d22\u5f15i\u5c42\u7684\u7ebf\u6027\u53d8\u6362\u7ed3\u679c\uff0cdelta[i]\u4ee3\u8868\u7d22\u5f15i\u5c42\u7684 \u53cd\u5411\u4f20\u64ad\u4fe1\u53f7\n",
      "        self.delta_record = [None for i in xrange(self.num_layers)]\n",
      "        self.s_record = [None for i in xrange(self.num_layers)]\n",
      "        self.h_record = [None for i in xrange(self.num_layers)]\n",
      "        self.setup_input()   \n",
      "\n",
      "    def setup_input(self):\n",
      "        \"\"\"\u8bbe\u7f6e\u6bcf\u4e00\u5c42\u7684\u8f93\u5165\u7a7a\u95f4\u7ef4\u6570\u5e76\u521d\u59cb\u5316\u6743\u503c\u77e9\u9635\"\"\"\n",
      "        self.layers[0].n_vis = self.n_vis\n",
      "        n_out = self.layers[0].n_hid\n",
      "        self.layers[0].W = self.numpy_rng.uniform(-self.irange, self.irange, (layers[0].n_vis, layers[0].n_hid))\n",
      "        #\u9010\u5c42\u914d\u7f6e\u8f93\u5165\u7a7a\u95f4\u7ef4\u6570\u5e76\u521d\u59cb\u5316\u6743\u503c\u77e9\u9635\n",
      "        for i in xrange(1, len(self.layers)):\n",
      "            #\u5176\u5b9e\u672c\u5c42\u5e94\u8be5\u7528layer[i].setup_input()\u4ee3\u66ff\n",
      "            self.layers[i].set_nvis(n_out)\n",
      "            self.layers[i].init_W(numpy_rng=self.numpy_rng, irange=self.irange)\n",
      "            n_out = self.layers[i].n_hid\n",
      "\n",
      "    def up_fresh(self, data):\n",
      "        \"\"\"\u8f93\u5165\u6b63\u5411\u4f20\u64ad\uff0c\u8ba1\u7b97\u6bcf\u4e00\u5c42\u7684\u8f93\u5165h\u548c\u4e2d\u95f4\u7ed3\u679cs\uff1b\u7d22\u5f15\u4e3ai\u7684\u5c42\u7684\u8f93\u5165\u662fh[i]\uff0c\u4e2d\u95f4\u7ed3\u679c\u662fs[i]\"\"\"\n",
      "        for i in xrange(self.num_layers):\n",
      "            layer = self.layers[i]\n",
      "            self.h_record[i] = data #\u8bb0\u5f55\u6bcf\u5c42h\n",
      "            s = layer.lin_propup(data)\n",
      "            self.s_record[i] = s #\u8bb0\u5f55\u6bcf\u5c42s\n",
      "            data = layer.f_propup(s)\n",
      "        return [self.s_record, self.h_record]\n",
      "\t\t\t\n",
      "    def down_fresh(self):\n",
      "        \"\"\"\u8bef\u5dee\u53cd\u5411\u4f20\u64ad\uff0c\u8ba1\u7b97\u6bcf\u5c42\u7684\u8bef\u5dee\u4fe1\u53f7delta\"\"\"\n",
      "        delta = self.delta_record[-1] #\u4e0e\u6b63\u5411\u4f20\u64ad\u4e0d\u540c\uff0c\u53cd\u5411\u4f20\u64ad\u7684\u521d\u59cb\u8f93\u5165\u8bef\u5dee\u4fe1\u53f7\u662f\u7531\u4ee3\u4ef7\u51fd\u6570\u4e0e\u6b63\u5411\u8f93\u5165\u786e\u5b9a\u7684\uff0c\u4e0d\u80fd\u4efb\u610f\u9009\u62e9\n",
      "        #cnt = 0\n",
      "         #\u6ce8\u610f1.\u53cd\u5411\u4f20\u64ad\u4ece\u6700\u540e\u4e00\u5c42\u5f00\u59cb\u5411\u524d\u4f20\u64ad\uff1b2.\u53cd\u5411\u4f20\u64ad\u7b2c2\u5c42\u548c\u7b2c\u4e00\u5c42\u4e4b\u95f4\u4e3a\u6b62\uff0c\u5373\u8bef\u5dee\u4fe1\u53f7\u4e0d\u4f1a\u7a7f\u8d8a\u8f93\u5165\u5c42-\u7b2c\u4e00\u9690\u5c42\u4e4b\u95f4\u7684\u77e9\u9635\n",
      "        for i in xrange(1, self.num_layers):\n",
      "            layer_up = self.layers[-i]\n",
      "            temp = layer_up.lin_propdown(delta)\n",
      "            layer_down = self.layers[-(i+1)]\n",
      "            delta = layer_down.f_propdown(temp, self.s_record[-(i+1)])\n",
      "            self.delta_record[-(i+1)] = delta #\u8bb0\u5f55\u6bcf\u5c42delta\n",
      "        return self.delta_record\n",
      "        \n",
      "    def train(self, dataset, labels, lr, batch_size, num_epochs, momentum=0.0):\n",
      "        \"\"\"\u6a21\u578b\u8bad\u7ec3\u51fd\u6570\"\"\"\n",
      "        assert isinstance(dataset, numpy.ndarray), isinstance(labels, numpy.ndarray)\n",
      "        assert dataset.ndim == 2, labels.ndim == 2\n",
      "        assert dataset.shape[0] == labels.shape[0]\n",
      "        num_samples, num_features = dataset.shape\n",
      "        num_batches = num_samples / batch_size\n",
      "        lr = float(lr)\n",
      " \n",
      "        #\u4e3a\u4e86\u52a8\u91cf\u521d\u59cb\u5316\n",
      "        for layer in self.layers:\n",
      "            layer.vW = numpy.zeros(shape=layer.W.shape, dtype=float)\n",
      "            layer.vb = numpy.zeros(shape=layer.b.shape, dtype=float)\n",
      "            \n",
      "        print \"learning_rate: %f\" % lr\n",
      "        print \"updates per epoch: %s | total updates: %s\" % (num_batches, num_batches*num_epochs)\n",
      "        likelihood_record = []\n",
      "        err_record = []\n",
      "        for epoch in xrange(num_epochs):\n",
      "            print \"[NN_NUMPY] Epoch\", epoch\n",
      "            err = []\n",
      "            likelihood = []\n",
      "            for index in xrange(num_batches):\n",
      "                data_batch = dataset[index*batch_size: (index+1)*batch_size]\n",
      "                label_batch = labels[index*batch_size: (index+1)*batch_size]\n",
      "                \n",
      "                #\u5237\u65b0\u6b63\u53cd\u5411\u4f20\u64ad\u8bb0\u5f55\n",
      "                self.up_fresh(data_batch)\n",
      "                self.delta_record[-1] = self.layers[-1].get_last_delta(s=self.s_record[-1], labels=label_batch)\n",
      "                self.down_fresh()\n",
      "                #cur_likelihood = self.layers[-1].log_likelihood(s=self.s_record[-1], labels=label_batch)\n",
      "                #cur_err = self.layers[-1].get_err(s=self.s_record[-1], labels=label_batch)\n",
      "                #likelihood.append(cur_likelihood)\n",
      "                #err.append(cur_err)\n",
      "                for i in xrange(self.num_layers):\n",
      "                    self.layers[i].backprop_update(h_l=self.h_record[i], delta_l=self.delta_record[i],\n",
      "                                                        lr=lr/float(batch_size), momentum=momentum)\n",
      "            for index in xrange(num_batches):\n",
      "                data_batch = dataset[index*batch_size: (index+1)*batch_size]\n",
      "                label_batch = labels[index*batch_size: (index+1)*batch_size]\n",
      "                self.up_fresh(data_batch)\n",
      "                \n",
      "                cur_likelihood = self.layers[-1].log_likelihood(s=self.s_record[-1], labels=label_batch)\n",
      "                cur_err = self.layers[-1].get_err(s=self.s_record[-1], labels=label_batch)\n",
      "                likelihood.append(cur_likelihood)\n",
      "                err.append(cur_err)\n",
      "                \n",
      "            likelihood_record.append(numpy.mean(likelihood))\n",
      "            err_record.append(numpy.mean(err))\n",
      "            print '    avg log-lh in this epoch: %f' % likelihood_record[-1]\n",
      "            print '    avg err-rate in this epoch: %f' % err_record[-1]\n",
      "            sys.stdout.flush()\n",
      "            \n",
      "class Layer(object):\n",
      "    \n",
      "    def __init__(self, n_hid):\n",
      "\n",
      "        self.n_hid = n_hid\n",
      "        self.b = numpy.zeros(n_hid, dtype=float)\n",
      "        \n",
      "    def f_propup(self, s):\n",
      "        \"\"\"\u6b63\u5411\u4f20\u64ad\u7684\u975e\u7ebf\u6027\u51fd\u6570\"\"\"\n",
      "        h = sigmoid(s)\n",
      "        return h\n",
      "    \n",
      "    def f_propdown(self, delta_h_l, s_l):\n",
      "        \"\"\"\u53cd\u5411\u4f20\u64ad\u901a\u8fc7\u975e\u7ebf\u6027\u51fd\u6570\"\"\"\n",
      "        f = d_sigmoid(s_l)\n",
      "        delta_l = delta_h_l * f\n",
      "        return delta_l\n",
      "    \n",
      "    def lin_propup(self, h):\n",
      "        \"\"\"\u7ebf\u6027\u53d8\u6362\u90e8\u5206\u6b63\u5411\u4f20\u64ad\"\"\"\n",
      "        assert hasattr(self, 'W')\n",
      "        #print 'h.shape:',h.shape\n",
      "        #print 'W.T.shape:', (self.W).shape\n",
      "        s = numpy.dot(h, self.W) + self.b\n",
      "        return s\n",
      "    \n",
      "    def lin_propdown(self, s):\n",
      "        \"\"\"\u7ebf\u6027\u53d8\u6362\u90e8\u5206\u53cd\u5411\u4f20\u64ad\"\"\"\n",
      "        assert hasattr(self, 'W')\n",
      "        #print 's.shape:', s.shape\n",
      "        #print 'W.T.shape:', (self.W.T).shape\n",
      "        h = numpy.dot(s, self.W.T)\n",
      "        return h\n",
      "        \n",
      "    def set_nvis(self, n_vis):\n",
      "        \"\"\"\u8bbe\u7f6e\u5f53\u524d\u5c42\u7684\u8f93\u5165\u7a7a\u95f4\u7ef4\u6570\"\"\"\n",
      "        self.n_vis = n_vis\n",
      "        \n",
      "    def init_W(self, numpy_rng, irange=0.1):\n",
      "        \"\"\"\u521d\u59cb\u5316\u5f53\u524d\u5c42\u6743\u503c\"\"\"\n",
      "        assert hasattr(self, 'n_vis')\n",
      "        self.W = numpy_rng.uniform(-irange, irange, (self.n_vis, self.n_hid))\n",
      "        \n",
      "    def backprop_update(self, delta_l, h_l, lr, momentum):\n",
      "        \"\"\"\u53cd\u5411\u4f20\u64ad\u7b97\u6cd5\"\"\"\n",
      "        gW = numpy.dot(h_l.T, delta_l)\n",
      "        gb = delta_l.sum(axis=0)\n",
      "        \n",
      "        assert numpy.sum(numpy.isnan(gW)) == 0\n",
      "        assert numpy.sum(numpy.isnan(gb)) == 0\n",
      "        \n",
      "        self.vW = self.vW * momentum + gW\n",
      "        self.vb = self.vb * momentum + gb\n",
      "        self.W += lr * self.vW\n",
      "        self.b += lr * self.vb\n",
      "        assert numpy.sum(numpy.isnan(self.W)) == 0\n",
      "        assert numpy.sum(numpy.isnan(self.b)) == 0\n",
      "        \n",
      "class SoftmaxLayer(Layer):\n",
      "    \"\"\"\u4f5c\u4e3a\u591a\u5c42\u795e\u7ecf\u7f51\u7edc\u7684\u6700\u540e\u4e00\u5c42\"\"\"\n",
      "    def __init__(self, n_hid, irange=0.5): #h_hid\u76f8\u5f53\u4e0en_classes\n",
      "        \"\"\"\u4ee3\u4ef7\u51fd\u6570\u5c42\uff0c\u5305\u542b\u4e00\u4e2a\u7ebf\u6027\u53d8\u6362\u548c\u4e00\u4e2a\u51fd\u6570\u8ba1\u7b97\uff0c\u8ba1\u7b97\u7ed3\u679c\u662f\u4e00\u4e2a\u6807\u91cf\"\"\"\n",
      "        #self.n_class = n_classes\n",
      "        self.n_hid = n_hid\n",
      "        self.b = numpy.zeros(n_hid, dtype=float)\n",
      "        \n",
      "    def cost(self, s):\n",
      "        \"\"\"\u4ee3\u4ef7\u51fd\u6570\u9009\u62e9softmax\u51fd\u6570\u8ba1\u7b97\u7684\u6982\u7387\u77e9\u9635\u7684\u6781\u5927\u4f3c\u7136\u51fd\u6570\uff0c\u7c7b\u6807\u4f7f\u7528one_hot\u578b\"\"\"\n",
      "        p_matrix = softmax(s)\n",
      "        predict_vector = numpy.argmax(p_matrix, axis=1) #\u53d6\u6bcf\u884c\u4e2d\u6982\u7387\u6700\u5927\u7684\u4e00\u4e2a\u4f5c\u4e3a\n",
      "        return predict_vector\n",
      "    \n",
      "    def get_err(self, s, labels):\n",
      "        \"\"\"\"\"\"\n",
      "        p_matrix = softmax(s)\n",
      "        predict = numpy.argmax(p_matrix, axis=1)\n",
      "        ground_truth = numpy.argmax(labels, axis=1)\n",
      "        return numpy.sum(predict != ground_truth) / float(labels.shape[0])\n",
      "    \n",
      "    def log_likelihood(self, s, labels):\n",
      "        \"\"\"\"\"\"\n",
      "        assert isinstance(labels, numpy.ndarray)\n",
      "        assert labels.ndim == 2\n",
      "        p_matrix = softmax(s)\n",
      "        sum_logp = (numpy.log(p_matrix) * labels).sum(axis=1).mean()\n",
      "        return - sum_logp\n",
      "    \n",
      "    def get_last_delta(self, s, labels): #labels\u662f\u4e00\u4e2a\u77e9\u9635\uff0c\u77e9\u9635\u7684\u6bcf\u4e00\u884c\u6570\u4e00\u4e2a\u6837\u672c\u7684one-hot\u7c7b\u6807\n",
      "        \"\"\"\u4f7f\u7528\u6781\u5927\u4f3c\u7136\u51fd\u6570\u4f5c\u4e3a\u4ee3\u4ef7\uff0c\u68af\u5ea6\u5411\u91cf\u5373\u4e3aone-hot\u7684\u7c7b\u6807\u5411\u91cf. \u6709\u76d1\u7763\u8bad\u7ec3\u7684delta\u5e94\u8be5\u4e0e\u7c7b\u6807\u6709\u5173\"\"\"\n",
      "        assert isinstance(labels, numpy.ndarray)\n",
      "        assert labels.ndim == 2\n",
      "        return labels* (1 - softmax(s))\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pylearn2.datasets.mnist import MNIST\n",
      "\n",
      "dsm_train = MNIST(which_set='train', start=0, stop=500, one_hot=True)\n",
      "dsm_valid = MNIST(which_set='train', start=500, stop=600, one_hot=True)\n",
      "dsm_test = MNIST(which_set='test', start=0, stop=10000, one_hot=True)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy\n",
      "import theano\n",
      "import theano.tensor as T\n",
      "\n",
      "#X = numpy.array([[1,0], [0, 1]], dtype=float)\n",
      "#y = numpy.array([[1, 0], [0, 1]], dtype=float)\n",
      "\n",
      "X = dsm_train.X#[:1000, :]\n",
      "y = dsm_train.y#[:1000, :]\n",
      "\n",
      "#layer0 = Layer(n_hid=500)\n",
      "#ayer1 = Layer(n_hid=500)\n",
      "layer2 = SoftmaxLayer(n_hid=10)\n",
      "\n",
      "layers = [layer2]\n",
      "#layers=[layer0, layer1, layer2]\n",
      "nn_model = NN(n_vis=784, layers=layers)\n",
      "\n",
      "nn_model.train(dataset=X, labels=y, lr=0.1, batch_size=50000, num_epochs=10, momentum=0.0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "learning_rate: 0.010000\n",
        "updates per epoch: 25 | total updates: 375\n",
        "[NN_NUMPY] Epoch 0\n",
        "    avg log-lh in this epoch: 3.238144"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    avg err-rate in this epoch: 0.934000\n",
        "[NN_NUMPY] Epoch 1\n",
        "    avg log-lh in this epoch: 2.904118"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    avg err-rate in this epoch: 0.932000\n",
        "[NN_NUMPY] Epoch 2\n",
        "    avg log-lh in this epoch: 2.715270"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    avg err-rate in this epoch: 0.922000\n",
        "[NN_NUMPY] Epoch 3\n",
        "    avg log-lh in this epoch: 2.609475"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    avg err-rate in this epoch: 0.926000\n",
        "[NN_NUMPY] Epoch 4\n",
        "    avg log-lh in this epoch: 2.547581"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    avg err-rate in this epoch: 0.920000\n",
        "[NN_NUMPY] Epoch 5\n",
        "    avg log-lh in this epoch: 2.508898"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    avg err-rate in this epoch: 0.922000\n",
        "[NN_NUMPY] Epoch 6\n",
        "    avg log-lh in this epoch: 2.483070"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    avg err-rate in this epoch: 0.910000\n",
        "[NN_NUMPY] Epoch 7\n",
        "    avg log-lh in this epoch: 2.465091"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    avg err-rate in this epoch: 0.902000\n",
        "[NN_NUMPY] Epoch 8\n",
        "    avg log-lh in this epoch: 2.452773"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    avg err-rate in this epoch: 0.892000\n",
        "[NN_NUMPY] Epoch 9\n",
        "    avg log-lh in this epoch: 2.445321"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    avg err-rate in this epoch: 0.886000\n",
        "[NN_NUMPY] Epoch 10\n",
        "    avg log-lh in this epoch: 2.442441"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    avg err-rate in this epoch: 0.860000\n",
        "[NN_NUMPY] Epoch 11\n",
        "    avg log-lh in this epoch: 2.443782"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    avg err-rate in this epoch: 0.834000\n",
        "[NN_NUMPY] Epoch 12\n",
        "    avg log-lh in this epoch: 2.448764"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    avg err-rate in this epoch: 0.812000\n",
        "[NN_NUMPY] Epoch 13\n",
        "    avg log-lh in this epoch: 2.456760"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    avg err-rate in this epoch: 0.802000\n",
        "[NN_NUMPY] Epoch 14\n",
        "    avg log-lh in this epoch: 2.467396"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    avg err-rate in this epoch: 0.806000\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}