{
 "metadata": {
  "name": "new_numpy_rbm"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "\u4f7f\u7528yield\u5236\u9020\u7684RBM\n",
      "\u5728\u4e24\u4e2a\u5730\u65b9\u4f7f\u7528yield\u5173\u952e\u5b57\u5236\u9020\u7c7b\u7684\u8fed\u4ee3\u5668\uff1a\n",
      "1.RBM\u7684gibbs\u62bd\u6837; 2.\u8bad\u7ec3\u6837\u672c\u96c6\u5728\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u65f6\u5faa\u73af\u4ea7\u751fmini_batch\n",
      "\n",
      "\u5173\u4e8ebug\uff1a\n",
      "\u4ece\u91cd\u6784\u8bef\u5dee\u503c\u770b\uff0c\u4f3c\u4e4e\u4ecd\u7136\u5b58\u5728\u95ee\u9898\uff1a\n",
      "k=1\u91cd\u6784\u8bef\u5dee\u503c\u504f\u5927\n",
      "k=2\u7684\u91cd\u6784\u8bef\u5dee\u5927\u4e8ek=1\u7684\u91cd\u6784\u8bef\u5dee\n",
      "\u4ee3\u7801\u6548\u7387\u4f4e\n",
      "\"\"\"\n",
      "import numpy\n",
      "import theano\n",
      "import theano.tensor as T\n",
      "from theano.tensor.shared_randomstreams import RandomStreams\n",
      "\n",
      "from pylearn2.datasets.mnist import MNIST\n",
      "\n",
      "assert theano.config.floatX == 'float64'\n",
      "\n",
      "def sigmoid(v):\n",
      "    \n",
      "    assert v.ndim == 2\n",
      "    x = T.matrix('x')\n",
      "    f = theano.function([x], T.nnet.sigmoid(x))\n",
      "    \n",
      "    return f(v)\n",
      "\n",
      "def theano_binomial(theano_rng):\n",
      "    \n",
      "    p = T.matrix()\n",
      "    f= theano.function([p], theano_rng.binomial(n=1, p=p, dtype=theano.config.floatX))\n",
      "    return f\n",
      "\n",
      "class RBM(object):\n",
      "    \n",
      "    def __init__(self, n_vis, n_hid):\n",
      "        \n",
      "        self.__dict__.update(locals())\n",
      "        del self.self\n",
      "        self.rng = numpy.random.RandomState(seed=19920130)\n",
      "        self.theano_rng = RandomStreams(seed=19920130)\n",
      "        self.W = self.rng.randn(n_vis, n_hid)\n",
      "        self.b = numpy.zeros(n_vis, dtype=theano.config.floatX)\n",
      "        self.c = numpy.zeros(n_hid, dtype=theano.config.floatX)\n",
      "        self.v_l = [[0., 0., 0.]]\n",
      "        self.f = theano_binomial(theano_rng=self.theano_rng)\n",
      "        \n",
      "    def get_params(self):\n",
      "        \n",
      "        return [self.W, self.c, self.b]\n",
      "    \n",
      "    def up(self, v, is_sample=True):\n",
      "        \"\"\"\u6b63\u5411\u4f20\u64ad, \u53c2\u6570is_sample\u7684\u76ee\u7684\u5b9e\u5728\u67d0\u4e9b\u4e0d\u9700\u8981\u8ba1\u7b97\u62bd\u6837\u503c\u7684\u60c5\u51b5\u4e0b\u51cf\u5c0f\u8ba1\u7b97\u91cf\"\"\"\n",
      "        h_mean = sigmoid(self.c + numpy.dot(v, self.W))\n",
      "        if is_sample is False:\n",
      "            return h_mean\n",
      "        h_sample = numpy.random.rand(*h_mean.shape) < h_mean\n",
      "        h_sample = numpy.cast[theano.config.floatX](h_sample)\n",
      "        #h_sample = self.f(h_mean)\n",
      "        \n",
      "        return [h_mean, h_sample]\n",
      "        \n",
      "    def down(self, h, is_sample=True):\n",
      "        \"\"\"\u53cd\u5411\u4f20\u64ad, \u53c2\u6570is_sample\u7684\u76ee\u7684\u5b9e\u5728\u67d0\u4e9b\u4e0d\u9700\u8981\u8ba1\u7b97\u62bd\u6837\u503c\u7684\u60c5\u51b5\u4e0b\u51cf\u5c0f\u8ba1\u7b97\u91cf\"\"\"\n",
      "        v_mean = sigmoid(self.b + numpy.dot(h, self.W.T))\n",
      "        if is_sample is False:\n",
      "            return v_mean\n",
      "        v_sample = numpy.random.rand(*v_mean.shape) < v_mean\n",
      "        v_sample = numpy.cast[theano.config.floatX](v_sample)\n",
      "        #v_sample = self.f(v_mean)\n",
      "        \n",
      "        return [v_mean, v_sample]\n",
      "            \n",
      "    def iter_vhv(self, v_sample):\n",
      "        \"\"\"\n",
      "        \u5bf9\u4e8e\u4e00\u4e2a\u5f53\u524d\u7c7b\u5b9e\u4f8bc\uff0cc.iter_pass_vhv()\u8fd4\u56de\u4e00\u4e2a\u8fed\u4ee3\u5668(iter\u5bf9\u8c61)\uff0c\u5bf9\u8fd9\u4e2aiter\u5bf9\u8c61\u53ef\u4ee5\u8c03\u7528.next()\u65b9\u6cd5\uff0c\u6765\u83b7\u5f97\u4e00\u6b21yield\u4ea7\u751f\u7684\u503c\n",
      "        \u7531\u4e8eiter_pass_vhv()\u7684\u5faa\u73af\u6ca1\u6709\u7ec8\u6b62\u6761\u4ef6\uff0c\u56e0\u6b64\u53ef\u4ee5\u4e0d\u505c\u7684\u91cd\u590d\u8c03\u7528next()\u65b9\u6cd5\u6765\u83b7\u5f97\u6cbf\u7740\u540c\u4e00\u6761Gibbs\u94fe\u4e0d\u65ad\u8fdb\u884c\u62bd\u6837\u7684\u6837\u672c\n",
      "        \"\"\"\n",
      "        while True:\n",
      "            h_mean, h_sample = self.up(v_sample)\n",
      "            v_mean, v_sample = self.down(h_sample)\n",
      "            yield [h_mean, h_sample, v_mean, v_sample]\n",
      "    \n",
      "    def gibbs_vhv(self, v_sample):\n",
      "        \"\"\"\u4f20\u7edf\u7684gibbs\u62bd\u6837\u65b9\u6cd5\uff0c\u4e0eyield\u505a\u6548\u7387\u5bf9\u6bd4\"\"\"\n",
      "        h_mean, h_sample = self.up(v_sample)\n",
      "        v_mean, v_sample = self.down(h_sample)\n",
      "        \n",
      "        return [h_mean, h_sample, v_mean, v_sample]\n",
      "        \n",
      "        \n",
      "            \n",
      "    def get_gradients(self, data):\n",
      "        \"\"\"\u8ba1\u7b97\u6a21\u578b\u4ee3\u4ef7\u51fd\u6570\u5173\u4e8e\u53c2\u6570\u7684\u5bfc\u6570\"\"\"\n",
      "        assert isinstance(data, (list, tuple))\n",
      "        assert len(data) == 4\n",
      "        \n",
      "        v_pos, h_pos, v_neg, h_neg = data\n",
      "        #batch_size = numpy.cast[theano.config.floatX](v_pos.shape[0])\n",
      "        grad_W = numpy.dot(v_pos.T, h_pos) - numpy.dot(v_neg.T, h_neg)\n",
      "        grad_b = v_pos.sum(axis=0) - v_neg.sum(axis=0)\n",
      "        grad_c = h_pos.sum(axis=0) - h_neg.sum(axis=0)\n",
      "        \n",
      "        return [grad_W, grad_c, grad_b]\n",
      "    \n",
      "    def update_params(self, grad, lr, momentum):\n",
      "        \n",
      "        gW, gc, gb = grad\n",
      "        #print '1:', gW.shape, '2:', gc.shape, '3:', gb.shape\n",
      "        v_m1 = self.v_l[-1]\n",
      "        self.W += momentum * v_m1[0] + lr * gW\n",
      "        #print 'bb:', self.b.shape\n",
      "        #print 'gb:', gb.shape\n",
      "        self.b += momentum * v_m1[2] + lr * gb\n",
      "        self.c += momentum * v_m1[1] + lr * gc\n",
      "        \n",
      "class Train(object):\n",
      "    \n",
      "    def __init__(self, model, dataset, lr, batch_size, momentum=0.0, k=1):\n",
      "        \n",
      "        self.k = k\n",
      "        self.model = model #\u6bcf\u4e2aTrain\u5b9e\u4f8b\u7ed1\u5b9a\u4e00\u4e2amodel\n",
      "        assert isinstance(dataset, numpy.ndarray)\n",
      "        assert dataset.ndim == 2\n",
      "        self.dataset = dataset\n",
      "        assert dataset.shape[0] % batch_size == 0 #\u8981\u6c42batch_size\u5fc5\u987b\u80fd\u88ab\u6837\u672c\u6570\u6574\u9664\n",
      "        self.batch_size = batch_size\n",
      "        self.lr = lr\n",
      "        self.momentum = momentum\n",
      "        self.cnt = 0 # \u6807\u8bb0\u6a21\u578b\u5df2\u7ecf\u8bad\u7ec3\u7684epoch\u6570\n",
      "        self.re_record = []\n",
      "    \n",
      "    def _deprecated_iter_dataset(self):\n",
      "        \n",
      "        idx = 0\n",
      "        while idx < self.dataset.shape[0]:\n",
      "            yield self.dataset[idx: idx+self.batch_size]\n",
      "            idx = idx + self.batch_size\n",
      "        \n",
      "    def train_one_epoch(self):\n",
      "        \"\"\"yield\u7248\u62bd\u6837\u5668\"\"\"\n",
      "        recon_error = []\n",
      "        for batch in self._deprecated_iter_dataset(): # \u8c03\u7528self.iter_dataset()\u6765\u751f\u6210\u4e00\u4e2a\u8bad\u7ec3\u6570\u636e\u96c6\u8fed\u4ee3\u5668\n",
      "            gibbs_sampler = self.model.iter_vhv(batch)\n",
      "            \n",
      "            pos_v_sample = batch\n",
      "            pos_h_mean, pos_h_sample, neg_v_mean, neg_v_sample = gibbs_sampler.next()\n",
      "            for i in xrange(self.k - 1): #\u5f53k>1\u65f6\uff0c\u8fed\u4ee3\u62bd\u6837\u66f4\u65b0neg_v\n",
      "                _, _, neg_v_mean, neg_v_sample = gibbs_sampler.next() #\u62bd\u6837\uff0c\u5f97\u5230v0, h0, v1, h1\n",
      "            del gibbs_sampler # \u5982\u679c\u91ca\u653egibbs_sampler\uff0c\u4f1a\u5bfc\u81f4\u7a0b\u5e8f\u5047\u6b7b\u673a\n",
      "            neg_h_mean = self.model.up(neg_v_sample, is_sample=False) #\u8ba1\u7b97neg_h\n",
      "            #pos_v_sample = batch\n",
      "            grad = self.model.get_gradients([pos_v_sample, \n",
      "                        pos_h_mean, neg_v_sample, neg_h_mean]) / numpy.cast[theano.config.floatX](self.batch_size) # \u8ba1\u7b97\u53c2\u6570\u68af\u5ea6\n",
      "            self.model.update_params(grad=grad, lr=self.lr, momentum=self.momentum)\n",
      "            #self.model.v_l.append(grad) # \u8bb0\u5f55\u672c\u6b21\u901f\u5ea6(\u68af\u5ea6)\n",
      "            recon_error.append(((neg_v_mean - pos_v_sample)**2).mean(axis=0).sum())\n",
      "        self.cnt += 1\n",
      "        recon_error = numpy.array(recon_error, dtype=theano.config.floatX).mean()\n",
      "        self.re_record.append(recon_error)\n",
      "        print 'epoch: %d, recon_error: %f' % (self.cnt, recon_error)\n",
      "        \n",
      "    def train_one_epoch_cmp(self):\n",
      "        \n",
      "        recon_error = []\n",
      "        for i in xrange(self.dataset.shape[0] / self.batch_size): # \u8c03\u7528self.iter_dataset()\u6765\u751f\u6210\u4e00\u4e2a\u8bad\u7ec3\u6570\u636e\u96c6\u8fed\u4ee3\u5668\n",
      "            batch = self.dataset[i*self.batch_size:(i+1)*self.batch_size, :]\n",
      "            pos_v_sample = batch\n",
      "            pos_h_mean, pos_h_sample, neg_v_mean, neg_v_sample = self.model.gibbs_vhv(pos_v_sample)\n",
      "            for i in xrange(self.k - 1): #\u5f53k>1\u65f6\uff0c\u8fed\u4ee3\u62bd\u6837\u66f4\u65b0neg_v\n",
      "                _, _, neg_v_mean, neg_v_sample = self.model.gibbs_vhv(neg_v_sample) #\u62bd\u6837\uff0c\u5f97\u5230v0, h0, v1, h1\n",
      "            neg_h_mean = self.model.up(neg_v_sample, is_sample=False) #\u8ba1\u7b97neg_h\n",
      "            grad = self.model.get_gradients([pos_v_sample, \n",
      "                        pos_h_mean, neg_v_sample, neg_h_mean]) / numpy.cast[theano.config.floatX](self.batch_size) # \u8ba1\u7b97\u53c2\u6570\u68af\u5ea6\n",
      "            self.model.update_params(grad=grad, lr=self.lr, momentum=self.momentum)\n",
      "            #self.model.v_l.append(grad) # \u8bb0\u5f55\u672c\u6b21\u901f\u5ea6(\u68af\u5ea6)\n",
      "            recon_error.append(((neg_v_mean - pos_v_sample)**2).mean(axis=0).sum())\n",
      "        self.cnt += 1\n",
      "        recon_error  = numpy.mean(recon_error)\n",
      "        self.re_record.append(recon_error)\n",
      "        print 'epoch: %d, recon_error: %f' % (self.cnt, recon_error)\n",
      "        \n",
      "if __name__ == '__main__':\n",
      "    \n",
      "    import time\n",
      "    dsm = MNIST(which_set='train', start=0, stop=500)\n",
      "    rbm = RBM(n_vis=784, n_hid=500)\n",
      "    \n",
      "    train = Train(model=rbm, dataset=dsm.X, lr=0.01, batch_size=20, momentum=0.0, k=1)\n",
      "    t = time.clock()\n",
      "    time_list = [t]\n",
      "    for i in xrange(15):\n",
      "        print 'start training epoch %d ...' % (i + 1)\n",
      "        #train.train_one_epoch()\n",
      "        train.train_one_epoch_cmp()\n",
      "        t = time.clock()\n",
      "        print '    time elapsed on this epoch is', t - time_list[-1]\n",
      "        time_list.append(t)\n",
      "    print 'toal time is', time_list[-1] - time_list[0]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "start training epoch 1 ...\n",
        "epoch: 1, recon_error: 201.601591"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    time elapsed on this epoch is 1.1\n",
        "start training epoch 2 ...\n",
        "epoch: 2, recon_error: 107.334628"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    time elapsed on this epoch is 1.12\n",
        "start training epoch 3 ...\n",
        "epoch: 3, recon_error: 81.851893"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    time elapsed on this epoch is 1.1\n",
        "start training epoch 4 ...\n",
        "epoch: 4, recon_error: 70.280263"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    time elapsed on this epoch is 1.09\n",
        "start training epoch 5 ...\n",
        "epoch: 5, recon_error: 63.286065"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    time elapsed on this epoch is 1.09\n",
        "start training epoch 6 ...\n",
        "epoch: 6, recon_error: 58.377362"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    time elapsed on this epoch is 1.09\n",
        "start training epoch 7 ...\n",
        "epoch: 7, recon_error: 54.653788"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    time elapsed on this epoch is 1.08\n",
        "start training epoch 8 ...\n",
        "epoch: 8, recon_error: 52.621351"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    time elapsed on this epoch is 1.1\n",
        "start training epoch 9 ...\n",
        "epoch: 9, recon_error: 50.522185"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    time elapsed on this epoch is 1.09\n",
        "start training epoch 10 ...\n",
        "epoch: 10, recon_error: 48.935852"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    time elapsed on this epoch is 1.09\n",
        "start training epoch 11 ...\n",
        "epoch: 11, recon_error: 47.785728"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    time elapsed on this epoch is 1.08\n",
        "start training epoch 12 ...\n",
        "epoch: 12, recon_error: 46.673420"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    time elapsed on this epoch is 1.08\n",
        "start training epoch 13 ...\n",
        "epoch: 13, recon_error: 45.779363"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    time elapsed on this epoch is 1.09\n",
        "start training epoch 14 ...\n",
        "epoch: 14, recon_error: 44.793261"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    time elapsed on this epoch is 1.09\n",
        "start training epoch 15 ...\n",
        "epoch: 15, recon_error: 43.893650"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "    time elapsed on this epoch is 1.08\n",
        "toal time is 16.37\n"
       ]
      }
     ],
     "prompt_number": 41
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "if __name__ == '__main__':\n",
      "    \n",
      "    dsm = MNIST(which_set='train', start=0, stop=500)\n",
      "    rbm = RBM(n_vis=784, n_hid=500)\n",
      "    \n",
      "    train = Train(model=rbm, dataset=dsm.X, lr=0.01, batch_size=20, momentum=0.0, k=5)\n",
      "    for i in xrange(15):\n",
      "        print 'start training epoch %d ...' % (i + 1) \n",
      "        train.train_one_epoch()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "start training epoch 1 ...\n",
        "epoch: 1, recon_error: 9.541077"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "start training epoch 2 ...\n",
        "epoch: 2, recon_error: 9.574081"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "start training epoch 3 ...\n",
        "epoch: 3, recon_error: 9.456668"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "start training epoch 4 ...\n",
        "epoch: 4, recon_error: 9.429082"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "start training epoch 5 ...\n",
        "epoch: 5, recon_error: 9.506178"
       ]
      },
      {
       "ename": "KeyboardInterrupt",
       "evalue": "",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-9-5b9310415a58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0;34m'start training epoch %d ...'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;32m<ipython-input-8-927868d89eee>\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mcnt\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0;31m#\u8fde\u7eed\u4e24\u6b21\u62bd\u6837\uff0c\u5f97\u5230v0, h0, v1, h1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m                 \u001b[0mpos_h_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_h_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_v_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_v_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgibbs_sampler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m                 \u001b[0mneg_h_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneg_v_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                 grad = self.model.get_gradients([pos_v_sample, \n",
        "\u001b[0;32m<ipython-input-8-927868d89eee>\u001b[0m in \u001b[0;36miter_vhv\u001b[0;34m(self, v_sample)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mh_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0mv_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mh_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_sample\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-8-927868d89eee>\u001b[0m in \u001b[0;36mdown\u001b[0;34m(self, h, is_sample)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_sample\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mv_mean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mv_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloatX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mv_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mv_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_sample\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "start training epoch 6 ...\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}